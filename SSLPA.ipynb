{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Tuple, Union\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "from cdlib import *\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from itertools import combinations,permutations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import jieba\n",
    "from zhon.hanzi import punctuation\n",
    "#jieba.load_userdict(\"\")\n",
    "import jieba.posseg as pseg\n",
    "import opencc\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora,models,similarities\n",
    "from collections import defaultdict,OrderedDict,Counter\n",
    "from pprint import pprint\n",
    "from gensim.models.word2vec import LineSentence,PathLineSentences\n",
    "from gensim.corpora import WikiCorpus\n",
    "import multiprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import  sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import RegexpStemmer\n",
    "import time\n",
    "import community\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from PIL import Image\n",
    "from PIL import ImageColor\n",
    "from imageio import imread,imsave\n",
    "from cdlib import evaluation as ev\n",
    "from cdlib import viz,readwrite,algorithms\n",
    "\n",
    "from community import modularity\n",
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "# import compare_algorithms as ca\n",
    "# plt.style.use('science')\n",
    "#忽略警告\n",
    "warnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "#同步更新版本\n",
    "class NodeOrder():\n",
    "    def __init__(self,G):\n",
    "        self.G=G\n",
    "    def get_adjacency_list(self):\n",
    "        # 获取邻居节点\n",
    "        adj_list={}\n",
    "        for nd in list(self.G.nodes()):\n",
    "            nei = list(self.G.neighbors(nd))\n",
    "            adj_list[nd] = nei\n",
    "        return adj_list\n",
    "    def default_order(self):\n",
    "        \"\"\"#the nodes are processed in the order of their original order in the graph\"\"\"\n",
    "\n",
    "        order=list(self.G.nodes())\n",
    "        return order\n",
    "    def random_order(self):\n",
    "        \"\"\"a random node-ordering is used\"\"\"\n",
    "        # print(list(self.G.nodes()))\n",
    "        nodes=list(self.G.nodes())\n",
    "\n",
    "\n",
    "        random.shuffle(nodes)\n",
    "        return nodes\n",
    "    def degree_order(self):\n",
    "        \"\"\"nodes are ordering by the degree of node\",\n",
    "        if some node have the same degree,ordering these nodes by its original exists order\"\"\"\n",
    "        order_list = sorted({v: self.G.degree(v) for v in list(self.G.nodes())}.items(), reverse=True,\n",
    "                            key=lambda k: k[1])\n",
    "        order =[l[0] for l in order_list]\n",
    "        c_1 = dict(Counter([c for n, c in self.G.degree()]))\n",
    "        return order,c_1\n",
    "    def degree_centrality_order(self):\n",
    "        \"\"\"nodes are ordering by the degree centrality of node\",\n",
    "        if some node have the same degree centrality,ordering these nodes by its original exists order\"\"\"\n",
    "        o=nx.degree_centrality(self.G)\n",
    "        order_list=sorted(o.items() ,reverse=True,key= lambda k:k[1])\n",
    "        order =[l[0] for l in order_list]\n",
    "        c_1 = dict(Counter([c for  c in o.values()]))\n",
    "        return order,c_1\n",
    "    def information_centrality_order(self):\n",
    "\n",
    "        \"\"\"nodes are ordering by the information_centrality of node\n",
    "        if some node have the same degree, ordering these nodes by its original exists order\"\"\"\n",
    "        o=nx.information_centrality(self.G)\n",
    "        order_list = sorted(o.items(), reverse=True, key=lambda k: k[1])\n",
    "        order = [l[0] for l in order_list]\n",
    "        c_1 = dict(Counter([c for c in o.values()]))\n",
    "        return order, c_1\n",
    "    def closeness_centrality_order(self):\n",
    "        \"\"\"nodes are ordering by the closeness_centrality of node\n",
    "        if some node have the same closeness centrality , ordering these nodes by its original exists order\"\"\"\n",
    "        o=nx.closeness_centrality(self.G)\n",
    "        order_list = sorted(o.items(), reverse=True, key=lambda k: k[1])\n",
    "        order = [l[0] for l in order_list]\n",
    "        c_1 = dict(Counter([c for c in o.values()]))\n",
    "        return order, c_1\n",
    "    def betweenness_centrality_order(self):\n",
    "        \"\"\"nodes are ordering by the betweenness_centrality of node\n",
    "        if some node have the same betweenness centrality, ordering these nodes by its original exists order\"\"\"\n",
    "        o = nx.betweenness_centrality(self.G)\n",
    "        order_list = sorted(o.items(), reverse=True, key=lambda k: k[1])\n",
    "        order = [l[0] for l in order_list]\n",
    "        c_1 = dict(Counter([c for c in o.values()]))\n",
    "        return order, c_1\n",
    "    def eigenvector_centrality_order(self):\n",
    "        \"\"\"nodes are ordering by the eigenvector_centrality of node\n",
    "        if some node have the same eigenvector_centrality , ordering these nodes by its original exists order\"\"\"\n",
    "        o = nx.eigenvector_centrality(self.G)\n",
    "        order_list = sorted(o.items(), reverse=True, key=lambda k: k[1])\n",
    "        order = [l[0] for l in order_list]\n",
    "        c_1 = dict(Counter([c for c in o.values()]))\n",
    "        return order, c_1\n",
    "    def katz_centrality_order(self):\n",
    "        \"\"\"nodes are ordering by the katz_centrality of node\n",
    "        if some node have the same katz_centrality, ordering these nodes by its original exists order\"\"\"\n",
    "        o=nx.katz_centrality(self.G)\n",
    "        order_list = sorted(o.items(), reverse=True, key=lambda k: k[1])\n",
    "        order = [l[0] for l in order_list]\n",
    "        c_1 = dict(Counter([c for c in o.values()]))\n",
    "        return order, c_1\n",
    "    def leadership_order(self):\n",
    "        #wei实现\n",
    "        \"\"\"nodes are ordering by the leader_ship of node\n",
    "        if some node have the same leader_ship, ordering these nodes by its original exists order\"\"\"\n",
    "        order_list = sorted(nx.leadership(self.G).items(), reverse=True, key=lambda k: k[1])\n",
    "\n",
    "        order = [l[0] for l in order_list]\n",
    "        return order\n",
    "    def subgraph_centrality_order(self):\n",
    "        \"\"\"nodes are ordering by the Alpha of node\n",
    "        if some node have the same Alpha, ordering these nodes by its original exists order\"\"\"\n",
    "        o = nx.subgraph_centrality(self.G)\n",
    "        order_list = sorted(o.items(), reverse=True, key=lambda k: k[1])\n",
    "        order = [l[0] for l in order_list]\n",
    "        c_1 = dict(Counter([c for c in o.values()]))\n",
    "        return order, c_1\n",
    "    def Alpha_order(self):\n",
    "        #未实现\n",
    "        \"\"\"nodes are ordering by the Alpha of node\n",
    "        if some node have the same Alpha, ordering these nodes by its original exists order\"\"\"\n",
    "        o={}\n",
    "        order_list = sorted([(v, self.G.degree(v)) for v in self.G.nodes()], reverse=True, key=lambda k: k[1])\n",
    "        order = [l[0] for l in order_list]\n",
    "        c_1 = dict(Counter([c for c in o.values()]))\n",
    "        return order, c_1\n",
    "    def PageRank_order(self):\n",
    "        \"\"\"nodes are ordering by the PageRank of node\n",
    "        if some node have the same PageRank, ordering these nodes by its original exists order\"\"\"\n",
    "        o=nx.pagerank(self.G)\n",
    "        order_list = sorted(o.items(), reverse=True, key=lambda k: k[1])\n",
    "        order = [l[0] for l in order_list]\n",
    "        c_1 = dict(Counter([c for c in o.values()]))\n",
    "        return order, c_1\n",
    "    def Hindex_order(self):\n",
    "        \"\"\"nodes are ordering by the Hindex of node\n",
    "        if some node have the same Hindex, ordering these nodes by its original exists order\"\n",
    "        as this method ,there may be a few node which have the same h-index,so it's not fit to order  nodes\"\"\"\n",
    "        h={}\n",
    "\n",
    "        for n in list(self.G.nodes()):\n",
    "            d=list(self.G.degree(list(self.G.neighbors(n))))\n",
    "            d=sorted(d,key=lambda x:x[1],reverse=True)\n",
    "            i=1\n",
    "            for node ,degree in d:\n",
    "                if i>degree:\n",
    "                    break\n",
    "                else:\n",
    "                    i=i+1\n",
    "            h[n]=i-1\n",
    "        order_list = sorted(h.items(), reverse=True, key=lambda k: k[1])\n",
    "        order = [l[0] for l in order_list]\n",
    "        c_1 = dict(Counter([c for c in h.values()]))\n",
    "        return order,c_1\n",
    "    def count_his(self):\n",
    "        #需将排序结果存入pandas中\n",
    "        o1,h1=self.degree_order()\n",
    "        o2,h2=self.degree_centrality_order()\n",
    "        # o3,h3=self.information_centrality_order()\n",
    "        o3, h3 = self.Hindex_order()\n",
    "        o4,h4=self.closeness_centrality_order()\n",
    "        o5,h5=self.betweenness_centrality_order()\n",
    "        o6,h6=self.eigenvector_centrality_order()\n",
    "        # o7,h7=self.katz_centrality_order()\n",
    "        o7, h7 =self.degree_order()\n",
    "        o8,h8=self.subgraph_centrality_order()\n",
    "        o9,h9=self.PageRank_order()\n",
    "\n",
    "        H=[h1,h2,h3,h4,h5,h6,h7,h8,h9]\n",
    "        O=[o1,o2,o3,o4,o5,o6,o7,o8,o9]\n",
    "        X=[s.keys() for s in H]\n",
    "        Y=[s.values() for s in H ]\n",
    "        sp=list(range(331,340,1))\n",
    "        T=['degree','degree_centrality','Hindex_order','closeness_centrality',\\\n",
    "           'betweenness_centrality','eigenvector_centrality',\\\n",
    "           'katz_centrality','subgraph_centrality','PageRank']\n",
    "\n",
    "\n",
    "        for i ,t,x,y, o ,h in zip(sp,T,X,Y,O,H):\n",
    "            print(t+\"排序结果\")\n",
    "            print(o)\n",
    "            print(t+\"指标频数统计\")\n",
    "            h=sorted(h.items(),key=lambda x:x[1],reverse=True)\n",
    "            print(h)\n",
    "            plt.subplot(i)\n",
    "            plt.xlabel(\"index_value\")\n",
    "            plt.ylabel(\"value_count\")\n",
    "            plt.title(t)\n",
    "            plt.bar(x, y)\n",
    "        plt.show()\n",
    "\n",
    "class WordProcess():\n",
    "    \"\"\"函数列表：获取文件列表get_filename、获取文件内容get_content、获取wiki语料\"\"\"\n",
    "    #数据预处理部分\n",
    "    def __init__(self,filein,fileout,model_file=None):\n",
    "        self.filein=filein\n",
    "        self.fileout=fileout\n",
    "        self.stoplist = []\n",
    "        self.model_file=model_file\n",
    "    def make_path(self,filepath):\n",
    "        if not os.path.exists(filepath):\n",
    "            os.mkdir(filepath)\n",
    "        return filepath\n",
    "\n",
    "\n",
    "    def get_filename(self,fileDir):\n",
    "        \"\"\"获取文件夹下的文件路径和名称，仅一层\"\"\"\n",
    "        filepath=[]\n",
    "        if os.path.exists(fileDir):\n",
    "            for filename in os.listdir(fileDir):\n",
    "                filepath.append(os.path.join(fileDir,filename))\n",
    "        return filepath\n",
    "    def get_filecontent(self,filename):\n",
    "        \"\"\"文本内容转化为列表，一行一列\"\"\"\n",
    "        #to [[],[]]\n",
    "        data=[]\n",
    "        with open(filename,'r',encoding='utf-8')as f:\n",
    "            for line in f.readlines():\n",
    "                data.append(line.strip(' ').strip('\\n').strip('\\t'))\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "    def merge_corpus(self,infilepath,output_file):\n",
    "        \"\"\"\"将多个text文件合并为一个并输出,每个文本为一个单元\"\"\"\n",
    "        data=[]\n",
    "        with open(self.fileout+output_file+'/.txt','w',encoding='utf-8')as f:\n",
    "            for filename in os.listdir(infilepath):\n",
    "                if '.txt' in filename:\n",
    "                    file_path = os.path.join(infilepath, filename)\n",
    "                    t = []\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\")as fr:\n",
    "\n",
    "                        for line in fr.readline():\n",
    "                            line = line.strip('\\n').strip('\\t')\n",
    "                            if len(line) > 0:\n",
    "                                t.append(line)\n",
    "\n",
    "                    s=' '.join(list(map(str,t)))\n",
    "                    f.writelines(s+'\\n')\n",
    "                    data.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "        return data\n",
    "    def write_matrix(self,filename,text):\n",
    "        #text=[]\n",
    "        #写入矩阵 [[],[]]\n",
    "\n",
    "        with open(filename,'w',encoding='utf-8')as f:\n",
    "            for l in text:\n",
    "                f.write(' '.join(l))\n",
    "                f.write('\\n')\n",
    "        return\n",
    "    def write_content(self,filename,text):\n",
    "        \"\"\"[[(),()],[(),()]]\"\"\"\n",
    "        with open(filename,'w',encoding='utf-8')as f:\n",
    "            for e in text:\n",
    "                for e1,e2 in e:\n",
    "                    f.write(str(e1)+':'+str(e2)+' ')\n",
    "                f.write('\\n')\n",
    "\n",
    "    def record_error(self, filename, errorcontent):\n",
    "        with open(filename, 'a', encoding='utf-8')as f:\n",
    "            f.write(errorcontent + '\\n')\n",
    "\n",
    "     #将下载下来的维基百科xml文件装化为txt文件\n",
    "    def parse_wiki_corpora(self,name):\n",
    "        wname=self.filein+name\n",
    "        with open(self.fileout+'wiki_text','w',encoding='utf-8')as f:\n",
    "            wiki = WikiCorpus(wname, lemmatize=False, dictionary={})\n",
    "            for text in wiki.get_texts():\n",
    "                f.write(' '.join(text) + \"\\n\")\n",
    "        return\n",
    "\n",
    "    #中文分词部分\n",
    "    #jieba分词\n",
    "    def add_dict_jieba(self,user_dict):\n",
    "        with open(self.fileout+user_dict,'r',encoding='utf-8')as f:\n",
    "            word=f.readline()\n",
    "            jieba.add_word(word.strip().strip('\\n').strip('\\t'))\n",
    "    def add_jieba_word(self,word,fre):\n",
    "        jieba.add_word(word,freq=fre)\n",
    "\n",
    "    def cfenci(self,doc,name):\n",
    "        \"\"\"输入每行是一个文本\"\"\"\n",
    "        new_doc=[]#[['',''],['','']]\n",
    "        #https://zhuanlan.zhihu.com/p/53521380\n",
    "        fileout=self.fileout+\"cfenci/\"\n",
    "        self.make_path(fileout)\n",
    "        with open(fileout+name+'.txt','w',encoding='utf-8')as f:\n",
    "            for line in doc:\n",
    "                word=[]\n",
    "                words = jieba.cut(line, cut_all=False)\n",
    "                for w in words:\n",
    "                    if w not in self.stoplist:\n",
    "                        word.append(w)\n",
    "                new_doc.append(w)\n",
    "                f.write(' '.join(word))\n",
    "                f.write('\\n')\n",
    "        print(\"{}文档分词完毕\".format(name))\n",
    "        return new_doc\n",
    "\n",
    "    def stopwords(self,stop_name):\n",
    "        stoplist=[]\n",
    "        with open(self.filein+stop_name,'r',encoding='utf-8')as f:\n",
    "            for s in f.readlines():\n",
    "                s=s.strip().strip('\\n').strip('\\t')\n",
    "                stoplist.append(s)\n",
    "            self.stoplist=stoplist\n",
    "        return self.stoplist\n",
    "\n",
    "    #繁转简体,去除无关内容\n",
    "    def get_content_convert(self,doc):\n",
    "        \"\"\"Conversions 转换参数\n",
    "        hk2s: Traditional Chinese (Hong Kong standard) to Simplified Chinese\n",
    "        s2hk: Simplified Chinese to Traditional Chinese (Hong Kong standard)\n",
    "        s2t: Simplified Chinese to Traditional Chinese\n",
    "        s2tw: Simplified Chinese to Traditional Chinese (Taiwan standard)\n",
    "        s2twp: Simplified Chinese to Traditional Chinese (Taiwan standard, with phrases)\n",
    "        t2hk: Traditional Chinese to Traditional Chinese (Hong Kong standard)\n",
    "        t2s: Traditional Chinese to Simplified Chinese\n",
    "        t2tw: Traditional Chinese to Traditional Chinese (Taiwan standard)\n",
    "        tw2s: Traditional Chinese (Taiwan standard) to Simplified Chinese\n",
    "        tw2sp: Traditional Chinese (Taiwan standard) to Simplified Chinese (with phrases)\"\"\"\n",
    "        text=[]\n",
    "        #regex_str=re.compile(\"[^<doc.*>$]|[^</doc>$]\")\n",
    "        cc = opencc.OpenCC('t2s')\n",
    "        with open(self.fileout+'opencc_convert.txt','w',encoding='utf-8')as f:\n",
    "            for c in doc:\n",
    "                c=c.strip('\\n').strip('\\t')\n",
    "                # 移除空行，移除其他字符\n",
    "                #regex_str.sub('',c)\n",
    "                if len(c)>0:\n",
    "                    c=cc.convert(c)\n",
    "                    text.append(c)\n",
    "                    f.write(c+'\\n')\n",
    "        return text\n",
    "    #英文分词\n",
    "\n",
    "    def word_cut(self,doc,name='efenci.txt'):\n",
    "        # stemmerlan = LancasterStemmer()\n",
    "        # stemmerporter = PorterStemmer()\n",
    "        # lemmatizer = WordNetLemmatizer()\n",
    "        stop_list=stopwords.words('english')\n",
    "        #去除标点符号后分词和不去除标点符号分词有区别吗？应该是没有\n",
    "        x = re.compile('[%s]' % re.escape(string.punctuation))  #去除标点符号\n",
    "\n",
    "        text=[]\n",
    "        for d in doc :\n",
    "            #统一为小写.去除标点符号\n",
    "            d= x.sub(u'', d.lower())\n",
    "            #避免有中文符号的影响 尤其是“”‘’\n",
    "            d = re.sub(\"[{}]+\".format(punctuation), \"\", d)\n",
    "            if d is not None:\n",
    "                w = self.lemmatize_all(d)\n",
    "                # 处理词形\n",
    "                word = [s for s in w if s not in stop_list]\n",
    "                # 处理词干，提取词干可能会有影响，慎重使用,如 apples 会被处理成appl\n",
    "                # word = [stemmerlan.stem(s) for s in word]\n",
    "                text.append(word)\n",
    "            else:\n",
    "                pass\n",
    "        self.write_matrix(self.fileout+name,text)\n",
    "        return text\n",
    "    #提取词干\n",
    "    def get_stem(self,doc):\n",
    "        stemerporter=PorterStemmer()\n",
    "        new_word=[]\n",
    "        for d in doc:\n",
    "            w=stemerporter.stem(d)\n",
    "            new_word.append(w)\n",
    "        return new_word\n",
    "    #词形\n",
    "    def lemmatize_all(self,sentence):\n",
    "        wnl = WordNetLemmatizer()\n",
    "        for word, tag in nltk.pos_tag(nltk.word_tokenize(sentence)):\n",
    "            if tag.startswith('NN'):\n",
    "                yield wnl.lemmatize(word, pos='n')\n",
    "            elif tag.startswith('VB'):\n",
    "                yield wnl.lemmatize(word, pos='v')\n",
    "            elif tag.startswith('JJ'):\n",
    "                yield wnl.lemmatize(word, pos='a')\n",
    "            elif tag.startswith('R'):\n",
    "                yield wnl.lemmatize(word, pos='r')\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def sentence_cut(self,doc):\n",
    "        #划分句子\n",
    "        text=sent_tokenize(doc)\n",
    "        return text\n",
    "    #向量化\n",
    "\n",
    "    def word_bag(self,texts,fre=0):\n",
    "        \"\"\"texts格式\n",
    "        [['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
    "        ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
    "        \"\"\"\n",
    "        # 去除低频词\n",
    "        frequency = defaultdict(int)\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                frequency[token] += 1\n",
    "        texts = [[token for token in text if frequency[token] > fre] for text in texts]\n",
    "        #输出词频\n",
    "        frequency=sorted(frequency.items(),reverse=True,key=lambda k:k[1])\n",
    "        with open(self.fileout+'all_word_frequence.txt','w',encoding='utf-8')as f:\n",
    "            json.dump(frequency,f,ensure_ascii=False,indent=4)\n",
    "        # bag-of-words\n",
    "        #pprint(texts[0:2])\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        dictionary.save(self.fileout+'worddictionary.dict')#store the dictonary ,for future reference\n",
    "        #输出词典词\n",
    "        # pprint(dictionary.token2id)\n",
    "        # 将其他文档转化为向量,如果新的文档中出现了不在词袋中的词，那么这些词将被舍弃\n",
    "        # new_doc = 'Human computer interaction'\n",
    "        # new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "        # print(new_vec)\n",
    "        # 将原始的文档向量化\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        #pprint(corpus)\n",
    "        # 保存向量化后的文档\n",
    "        corpora.MmCorpus.serialize(self.fileout+'corpus.mm', corpus)\n",
    "        return (corpus,dictionary)\n",
    "\n",
    "    def read_content(self,dict_file,corpus_file):\n",
    "        print(\"读取词典数据和向量数据\")\n",
    "        # 词典数据\n",
    "        dictionary = corpora.Dictionary.load(dict_file)\n",
    "        # 向量数据\n",
    "        corpus = corpora.MmCorpus(corpus_file)\n",
    "        return (dictionary,corpus)\n",
    "\n",
    "    def word_tfidf(self,corpus):\n",
    "        tfidf = models.TfidfModel(corpus,normalize=True)  # initialize a model\n",
    "        # transform new document,use the model to transform vectors\n",
    "        # apply the model to a whole corpus,in this case,just use the same corpus ,\n",
    "        # Once the transformation model has been initialized, it can be used on any vectors\n",
    "        # (provided they come from the same vector space, of course), even if they were not used in the training corpus at all\n",
    "        # Calling model[corpus] only creates a wrapper around the old corpus document stream – actual conversions are done on-the-fly\n",
    "\n",
    "        corpus_tfidf = tfidf[corpus]\n",
    "        corpus_tfidf.save(self.fileout+'model.tfidf')\n",
    "        # when this done ,can save the results,see 语料库的保存\n",
    "        # We cannot convert the entire corpus at the time of calling corpus_transformed = model[corpus],\n",
    "        #for doc in corpus_tfidf:\n",
    "            #print(doc)\n",
    "        return corpus_tfidf\n",
    "    def word_lsi(self,curpos_tfidf,dictionary,num_topic):\n",
    "        # Transformations can also be serialized, one on top of another, in a sort of chain\n",
    "        # Here we transformed our Tf-Idf corpus via Latent Semantic Indexing into a latent 2-D space (2-D because we set num_topics=2)\n",
    "        lsi_model = models.LsiModel(curpos_tfidf, id2word=dictionary, num_topics=num_topic)\n",
    "        lsi_model.save(self.fileout+'model.lsi')\n",
    "        return lsi_model\n",
    "    def word_lda(self,corpus,dictionary):\n",
    "        lda_model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)\n",
    "        lda_model.save(self.fileout+'model.lda')\n",
    "        return lda_model\n",
    "    def train_word2vec_single(self,input_file,size=256, window=10, min_count=5):\n",
    "        #一行为一个文本,可以直接为txt文件名\n",
    "        sentence=LineSentence(input_file)\n",
    "        #防止语料过大无法加载\n",
    "        # 训练模型\n",
    "        model=Word2Vec(sentence,size=size,window=window,min_count=min_count,workers=multiprocessing.cpu_count(),iter=10)\n",
    "        model.save(self.fileout+'wiki_corpus.model')\n",
    "        model.wv.save_word2vec_format(self.fileout+'word2vec_format',binary=False)\n",
    "        return model\n",
    "    def train_word2vec_dir(self,input_dir,size=256, window=10, min_count=5):\n",
    "\n",
    "\n",
    "       # 输入语料目录:PathLineSentences(input_dir)，input_dir为语料目录，而不是文件名\n",
    "       #embedding size:256 共现窗口大小:10 去除出现次数5以下的词,多线程运行,迭代10次\n",
    "        model = Word2Vec(PathLineSentences(input_dir), size=size, window=window, min_count=min_count, workers=multiprocessing.cpu_count(), iter=10)\n",
    "\n",
    "        model.save(self.fileout + 'wiki_corpus.model')\n",
    "        model.wv.save_word2vec_format(self.fileout + 'word2vec_format', binary=False)\n",
    "        return model\n",
    "    # 一次性加载语料\n",
    "    def train_word2vec_filter(self):\n",
    "        return\n",
    "    def get_word_word2vector(self,words,model,k=300):\n",
    "        #需确保words是不重复的\n",
    "        word_vector=defaultdict()\n",
    "        i=0\n",
    "        for w in words :\n",
    "            if w in model.wv.vocab:\n",
    "                word_vector[w]=model.wv[w]\n",
    "            else:\n",
    "                i+=1\n",
    "                word_vector[w]=np.random.uniform(-0.25,0.25,k)\n",
    "        print(\"未登录词数为{}\".format(i))\n",
    "        return word_vector\n",
    "    \"\"\"关于归一化：\n",
    "       因为余弦值的范围是 [-1,+1] ，相似度计算时一般需要把值归一化到 [0,1]，一般通过如下方式：\n",
    "       sim = 0.5 + 0.5 * cosθ\n",
    "       若在欧氏距离公式中，取值范围会很大，一般通过如下方式归一化：\n",
    "       sim = 1 / (1 + dist(X,Y))\"\"\"\n",
    "    def vector_cosine(self,v1,v2):\n",
    "        d1=np.asarray([v1])\n",
    "        d2=np.asarray([v2])\n",
    "        num=float(d1.dot(d2.T))\n",
    "        denom=np.linalg.norm(d1)*np.linalg.norm(d2)\n",
    "        cos=num/denom #余弦值\n",
    "        sim=0.5+0.5*cos#根据皮尔逊相关系数归一化\n",
    "        return sim\n",
    "    def vector_euclidean(self,v1,v2):\n",
    "        d1 = np.asarray([v1])\n",
    "        d2 = np.asarray([v2])\n",
    "        dist = np.linalg.norm(d1 - d2)\n",
    "        sim = 1.0 / (1.0 + dist)  # 归一化\n",
    "        return sim\n",
    "\n",
    "\n",
    "\n",
    "    def get_text_bagvector(self,dictionary,curpus):\n",
    "        \"\"\"取出每个句子的词\",not for word2vec\n",
    "        curpus格式\n",
    "        [[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
    "        [(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555)]]\n",
    "        text格式：每行为词和词的频率\n",
    "        [[(),(),()],[(),()]]\"\"\"\n",
    "        text=[]\n",
    "        dict=dictionary.token2id\n",
    "        #dict格式：dict of(str,int)\n",
    "        for d in curpus:\n",
    "            doc=[]\n",
    "            for w ,f in d:\n",
    "                # 比较id，获得词的频率\n",
    "                t=[k for (k,v) in dict.items() if v==w][0]\n",
    "                doc.append((t,f))\n",
    "            doc.sort(key=lambda x: x[1], reverse=True)  # 进行排序\n",
    "            \"\"\"\"\n",
    "            #取出规定长度的词\n",
    "            if len(doc)<list_length:\n",
    "                pass\n",
    "            else:\n",
    "                doc=doc[0:list_length]\"\"\"\n",
    "            text.append(doc)\n",
    "        self.write_content(self.fileout+'word_frequence.txt',text)\n",
    "        return text\n",
    "\n",
    "    def load_word2vec(self):\n",
    "        #加载预训练模型，如果模型中没有这个词，应当如何处理?\n",
    "        # model = models.Word2Vec.load(self.model_file)\n",
    "        model_file=self.model_file\n",
    "        model=models.KeyedVectors.load_word2vec_format(model_file,binary=True)\n",
    "        return model\n",
    "    def wsimilarity_top_word(self,model,word,top=10):\n",
    "        #获取最相似的词\n",
    "        sim_word=model.most_similar(word,topn=top)\n",
    "        return sim_word\n",
    "    def words_similarity(self,w1,w2,model):\n",
    "\n",
    "        w1_new=[w for w in w1 if w in model.wv.vocab]\n",
    "        w2_new=[w for w in w2 if w in model.wv.vocab]\n",
    "        if len(w1)!=len(w1_new) or len(w2)!=len(w2_new):\n",
    "            self.record_error(self.fileout+'not_in_word2vec_vocab.txt',\"原句1：{},新句1:{};原句2:{},新句2{}\".format(w1,w1_new,w2,w2_new))\n",
    "        #比较两个词列表的相似性\n",
    "        if len(w1_new)!=0 and len(w2_new)!=0:\n",
    "            sim=model.wv.n_similarity(w1_new,w2_new)\n",
    "        else:\n",
    "            print(w1,w2)\n",
    "            sim=0\n",
    "        return sim\n",
    "\n",
    "    def compare_word_similarity(self,model,word_list):\n",
    "        word_pair_sim=[]\n",
    "        with open(self.fileout+\"word_pair_similarity.txt\",'a',encoding='utf-8')as f:\n",
    "            for b_word, e_word in word_list:\n",
    "                res = model.similarity(b_word, e_word)\n",
    "                w_p_s=set(b_word, e_word, res)\n",
    "                word_pair_sim.append(w_p_s)\n",
    "                f.write(w_p_s)\n",
    "                f.write('\\n')\n",
    "        return word_pair_sim\n",
    "\n",
    "class GraphProcess():\n",
    "    \"\"\"build and clean the Graph\"\"\"\n",
    "    def  __init__(self,filein,fileout):\n",
    "        self.filein=filein\n",
    "        self.fileout=fileout\n",
    "\n",
    "    def add_node_attribute(self,G,n_a,att_name):\n",
    "        #n_a is a {node_id:att},if att is a dict ,then the key of att will be the attribute' name\n",
    "        if isinstance(dict,n_a):\n",
    "            nx.set_node_attributes(G,n_a,att_name)\n",
    "            print(\"add attribute to node success\")\n",
    "        return G\n",
    "    def add_edge_attribute(self,G,e_a,att_name):\n",
    "        #e_a must be {(n1,n2):att} ,att is the same as before\n",
    "        if isinstance(dict,e_a):\n",
    "            nx.set_node_attributes(G,e_a,att_name)\n",
    "            print(\"add attribute to edge success\")\n",
    "        return G\n",
    "\n",
    "    #按照度清洗节点\n",
    "    def isloated(self,G):\n",
    "        G1=G.copy()\n",
    "        #remove only degree-zero nodes\n",
    "        #Note:if remove the node,please remove the node attribute at the same time\n",
    "        s=list(nx.isolates(G1))\n",
    "        G1.remove_nodes_from(s)\n",
    "        skip_node=list(map(str,s))\n",
    "        nx.write_gexf(G1, self.fileout + 'clean_0_Graph.gexf', encoding='utf-8')\n",
    "        return G1, skip_node\n",
    "\n",
    "    def prune(self,g, limit=2):\n",
    "        G=g.copy()\n",
    "        # remove nodes which have  too few neighbors\n",
    "        skip=[]\n",
    "        for v in list(G.nodes()):\n",
    "            d = G.degree(v)\n",
    "            if d < limit:\n",
    "                skip.append(v)\n",
    "        if len(skip) > 0:\n",
    "            G.remove_nodes_from(skip)\n",
    "            skip = list(map(str, skip))\n",
    "            nx.write_gexf(G, self.fileout + 'clean_{}_Graph.gexf'.format(limit), encoding='utf-8')\n",
    "        return G, skip\n",
    "    def neighbor_hop(self,G,hop=0):\n",
    "        neighbors=defaultdict(lambda :'N/A')\n",
    "        #{id:[]}\n",
    "        \"\"\"definite the neighborhood ,hop means the range of neighbors\n",
    "        hop=0: the neighbors as well as the node itself,0-1 hop\n",
    "        hop=1:only the direct neighbors of the node  ,1 hop\n",
    "        hop=2: neighbors of the neighbors and include the node itself ,2 hop\"\"\"\n",
    "        for node in list(G.nodes()):\n",
    "            nei=[]\n",
    "            #包含0节点时节点自自己和自己的相似性为1，会放大自身标签传播的可能\n",
    "            if hop == 0:\n",
    "                # print(\"0-1 hop\")\n",
    "                nei = list(G.neighbors(node)) + [node]\n",
    "            elif hop == 1:\n",
    "                # print(\"1 hop\")\n",
    "                nei = list(G.neighbors(node))\n",
    "            #与0-1 hop 同样的问题\n",
    "            elif hop == 2:\n",
    "                # print(\"2 hop\")\n",
    "                nei1 = list(G.neighbors(node))\n",
    "                nei=nei1+[node]\n",
    "                for node in nei1:\n",
    "                    nei2 = list(G.neighbors(node))\n",
    "                    nei=nei+nei2\n",
    "            node=str(node)\n",
    "            nei=list(map(str,nei))\n",
    "            neighbors[node] = nei\n",
    "        return neighbors\n",
    "    def visual_G(self,G,c_G):\n",
    "        print(\"绘制原始图及处理后图\")\n",
    "        plt.figure(figsize=(10,40),dpi=500)\n",
    "        plt.subplot(411)\n",
    "        nx.draw_networkx(G,nx.spring_layout(G))\n",
    "        plt.axis('off')\n",
    "        plt.title('Origin Graph')\n",
    "\n",
    "        plt.subplot(412)\n",
    "        d_g_o = nx.degree_histogram(G)\n",
    "        d_g_o_s = pd.Series(d_g_o)\n",
    "        d_g_o_s.to_csv(self.fileout + 'degree_distribution_graph.txt')\n",
    "        x = np.arange(0, len(d_g_o), 1)\n",
    "        plt.bar(x, d_g_o)\n",
    "        plt.title('The degree distribution of origin graph')\n",
    "        plt.xlim(0,len(x)+1)\n",
    "        plt.ylim(0,max(d_g_o)+1)\n",
    "        plt.xticks(np.linspace(0,len(x),num=10,endpoint=True))\n",
    "        plt.yticks(np.linspace(0, max(d_g_o), num=10, endpoint=True))\n",
    "        plt.xlabel('degree')\n",
    "        plt.ylabel('degree count')\n",
    "\n",
    "        print(\"原始图信息\")\n",
    "        print(nx.info((G)))\n",
    "        print(\"原始图度分布\")\n",
    "        print(d_g_o)\n",
    "        plt.subplot(413)\n",
    "        print(nx.info(c_G))\n",
    "        nx.draw_networkx(c_G,pos=nx.spring_layout(c_G))\n",
    "        plt.axis('off')\n",
    "        plt.title('Processed Graph')\n",
    "        plt.subplot(414)\n",
    "        d_g = nx.degree_histogram(c_G)\n",
    "        d_g_s=pd.Series(d_g)\n",
    "        d_g_s.to_csv(self.fileout+'degree_distribution_processed_graph.txt')\n",
    "        x = np.arange(0, len(d_g), 1)\n",
    "        plt.bar(x, d_g)\n",
    "        plt.title('The degree distribution of processed graph')\n",
    "        plt.xlim(0, len(x)+1)\n",
    "        plt.ylim(0, max(d_g)+1)\n",
    "        plt.xticks(np.linspace(0, len(x), num=10, endpoint=True))\n",
    "        plt.yticks(np.linspace(0, max(d_g), num=10, endpoint=True))\n",
    "        plt.xlabel('degree')\n",
    "        plt.ylabel('degree count')\n",
    "\n",
    "        print(\"删除部分节点后图信息\")\n",
    "        print(nx.info(c_G))\n",
    "        print(\"删除部分节点后图度分布\")\n",
    "        print(d_g)\n",
    "        plt.savefig(self.fileout+\"Graph information.png\",dpi=500)\n",
    "        plt.show()\n",
    "class SlpaExtend():\n",
    "    \"\"\"A program to identify the overlapping community and overapping node which combine\n",
    "    the semantic content of the node and topology of the network.The target of the program is\n",
    "     to get  communities with specific meaning\n",
    "      Note: all the data is clean ,it dosen't need to process again\"\"\"\n",
    "    def __init__(self,filein,fileout):\n",
    "        \"\"\"Parameter:\n",
    "                Iteration:number of iterations\n",
    "                Threshold :t in [0,1],if the probability of the label is less than t,remove it\n",
    "                SimThreshold: if the similarity between two nodes is less than SimThreshold,The listener\n",
    "                won't accept the label of the spreaker\n",
    "                Attcoeff:attenuation coefficient of the label during iteration\"\"\"\n",
    "        self.filein=filein\n",
    "        self.fileout=fileout\n",
    "        #存储label  {t:{node_id:[(label,pro)]}}\n",
    "        self.node_memory={}\n",
    "        # 存储id  {t:{node_id:[(node,pro)]}}\n",
    "        self.accept_id={}\n",
    "        #modularity  {t:m}\n",
    "        self.modu={}\n",
    "        self.node_similarity=defaultdict()\n",
    "        self.com_count={}\n",
    "        self.com_iter_nodecount=defaultdict()\n",
    "        self.node_iter_comcount=defaultdict()\n",
    "\n",
    "        #the data structure of node_memory :{iterid:{node_id:[(label,propority),()]},{}},iterid in [0,T]\n",
    "        #origin_label  means the original label of nodes when initialize\n",
    "        #previous_label means the label of node in the previous iteration\n",
    "        #current_label represent the label of node in current iteration\n",
    "        # self.config={'max_Iteration':Iteration,'similarity_Threshold':Sim_Threshold,\\\n",
    "        #              'hop':hop,'stop_condition':stop_condition,\\\n",
    "        #              'stop_Threshold':stop_Threshold,'cut_threshold':cut_threshold}\n",
    "        \"\"\"data structure\n",
    "        adjacency_list:{id:[nei_id1,nei_id2]} for each node a list of its neighbours id\n",
    "        node_memory:{id:[(label,probability)]} fro each node a list of label and label's probability\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    def get_node_label_matrix(self, node_label):\n",
    "        label_matrix = []\n",
    "        for label in node_label.values():\n",
    "            label_matrix.append(label)\n",
    "        return label_matrix\n",
    "\n",
    "    def get_node_content(self,Idcontent_file):\n",
    "        # 默认文件格式是id:content,建立节点的id,content对照表\n",
    "        node_content =OrderedDict()\n",
    "        with open(Idcontent_file, 'r',encoding='utf-8')as f:\n",
    "            for c in f.readlines():\n",
    "                c=c.strip().strip('\\n').strip('\\t').split(\":\")\n",
    "                #防止出现多个冒号\n",
    "                content=' '.join(c[1:len(c)])\n",
    "                id=str(c[0])\n",
    "                node_content[id]=content\n",
    "        return (node_content)\n",
    "    def prune_node_content(self,skip_node,node_content):\n",
    "        del_node_content={}\n",
    "        if skip_node is not None:\n",
    "            #防止int 不能识别\n",
    "\n",
    "            for id in skip_node:\n",
    "                del_node_content[id]=node_content[id]\n",
    "                del node_content[id]\n",
    "            self.write_json_file(del_node_content,self.fileout+'drop_node_content.json')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return node_content\n",
    "    def get_node_label(self,i,id):\n",
    "        label=defaultdict()\n",
    "        for l,p in self.node_memory[i][id]:\n",
    "            label.append(l)\n",
    "        return label\n",
    "\n",
    "    def get_node_content_list(self, node_content):\n",
    "        content_list = []\n",
    "        for content in node_content.values():\n",
    "            content_list.append(content)\n",
    "        return content_list\n",
    "    def get_doc_word(self,wordprocess,node_content,Languagetag=0):\n",
    "        \"\"\"\"处理未分词的文本\n",
    "        node_content的形式是{node_id:''}\"\"\"\n",
    "        content = self.get_node_content_list(node_content)\n",
    "        if Languagetag == 0:\n",
    "            # 英文分词\n",
    "            doc = wordprocess.word_cut(content)\n",
    "        else:\n",
    "            # 中文繁体转简体\n",
    "            doc1 = wordprocess.get_content_convert(content)\n",
    "            # 中文分词\n",
    "            doc = wordprocess.cfenci(doc1)\n",
    "        return doc\n",
    "    def get_doc_word_done(self,node_content):\n",
    "        \"\"\"\"将已经分过词的句子转化为[['',''],['','']]\"\"\"\n",
    "        doc=[]\n",
    "        for content in node_content.values():\n",
    "            c=content.split(' ')\n",
    "            doc.append(c)\n",
    "        return doc\n",
    "\n",
    "    def initialize_label(self,wordprocess,node_content,Languagetag=0):\n",
    "        \"\"\"默认处理英文，Languagetag=0;处理中文时Languagetag=1\n",
    "\n",
    "        初始化标签时对标签的分布做了标准化，即p/sum(p),转化为标签的初始概率分布\n",
    "        再之后的迭代中每一轮都进行标准化吗？\"\"\"\n",
    "        #node_label格式 {id:[]}\n",
    "        #初始化标签\n",
    "        #origin_label格式：{id:[(),()]}\n",
    "        origin_label={}\n",
    "        accept_id={}\n",
    "        #对于未分词的原始文档，进行如下处理\n",
    "        # print(node_content)\n",
    "        doc=self.get_doc_word(wordprocess,node_content,Languagetag)\n",
    "        # 对于已经分过词的，进行如下处理\n",
    "        # doc=self.get_doc_word_done(node_content)\n",
    "        # 转换为词袋\n",
    "        curpus,dictionary=wordprocess.word_bag(doc)\n",
    "        #转换为tf-idf\n",
    "        corpus_tfidf=wordprocess.word_tfidf(curpus)\n",
    "        #\n",
    "        word_text=wordprocess.get_text_bagvector(dictionary,corpus_tfidf)\n",
    "\n",
    "        for id, word_fre in zip(node_content.keys(),word_text):\n",
    "            origin_label[id] = word_fre\n",
    "            accept_id[id]=[(id,1)]\n",
    "        filepath = self.make_filedir(self.fileout + 'original_label/')\n",
    "        self.write_nodel_label(origin_label,filepath,'origin_label')\n",
    "        origin_label_norm=self.label_frequency_normalize(origin_label)\n",
    "        # origin_label_norm=origin_label\n",
    "\n",
    "        self.write_nodel_label(origin_label_norm, filepath,'origin_label_normal')\n",
    "        self.node_memory[0] =origin_label_norm\n",
    "        self.accept_id[0]=accept_id\n",
    "        return origin_label_norm,accept_id\n",
    "    def label_frequency_normalize(self,n_label):\n",
    "        #n_label  {id:[(id,pre)]}\n",
    "        #需要优化数据结构\n",
    "        node_label={}\n",
    "        for id, word_fre in n_label.items():\n",
    "            # print(word_fre)\n",
    "            f_sum = sum([p for l,p in word_fre])\n",
    "            sen: List[Tuple[Any, Union[float, Any]]] = []\n",
    "            for label, fre in word_fre:\n",
    "                sf=fre/f_sum\n",
    "                sen.append((label, sf))\n",
    "            sen=sorted(sen,key=lambda x: x[1], reverse=True)  # 进行排序\n",
    "            node_label[id] = sen\n",
    "            #如果出现节点的label为空怎么处理？\n",
    "        return node_label\n",
    "    def cut_label(self,n_label,cut_thre):\n",
    "        # 如果选择过滤，度越大的节点接收到的标签种类越多，每个标签的fre较低，会被全部过滤\n",
    "        # 如果sf小于一定值会被过滤掉，剩下的值之和不为1,还需要二次处理使得节点所属标签值和为1.需要两次归一化?\n",
    "        node_label = defaultdict()\n",
    "        #取一个固定的cut阈值\n",
    "        for id, word_fre in n_label.items():\n",
    "            sen=[(label,fre) for label ,fre in word_fre if fre>= cut_thre]\n",
    "            #如果没有满足的，取最大的一个\n",
    "            if len(sen)==0:\n",
    "                sen=[sorted(word_fre,key=lambda x:x[1],reverse=True)[0]]\n",
    "            sen=sorted(sen,key=lambda x: x[1], reverse=True)  # 进行排序\n",
    "            node_label[id] = sen\n",
    "            # 如果出现节点的label为空怎么处理？\n",
    "\n",
    "        return node_label\n",
    "    def cut_label_change(self,n_label,per=25):\n",
    "        node_label = defaultdict()\n",
    "        # 根据节点拓扑结构取cut阈值\n",
    "        #暂定为四分位数\n",
    "\n",
    "        for id, word_fre in n_label.items():\n",
    "            cut_threshold=np.percentile([ fre for label, fre in word_fre],per)\n",
    "            sen = [(label, fre) for label, fre in word_fre if fre >= cut_threshold]\n",
    "            # 如果没有满足的，取最大的一个,在这里其实没有必要\n",
    "            if len(sen) == 0:\n",
    "                sen = [sorted(word_fre, key=lambda x: x[1], reverse=True)[0]]\n",
    "            sorted(sen,key=lambda x: x[1], reverse=True)  # 进行排序\n",
    "            node_label[id] = sen\n",
    "            # 如果出现节点的label为空怎么处理？\n",
    "\n",
    "        return node_label\n",
    "\n",
    "    def cut_model(self, n_label_pro, n_accept_id, c_model=1, per=25,cut_threshold=0.05):\n",
    "        print(\"cut_threshold:{}\".format(cut_threshold))\n",
    "        # 先标准化再切割？\n",
    "        new_label_pro=n_label_pro\n",
    "        new_accept_id=n_accept_id\n",
    "        node_memory={}\n",
    "        accept_id={}\n",
    "        if c_model == 0:\n",
    "            # 只归一化，不做其他处理\n",
    "            node_memory = self.label_frequency_normalize(new_label_pro)\n",
    "            accept_id = self.label_frequency_normalize(new_accept_id)\n",
    "        elif c_model == 1:\n",
    "            # 对所有标签按照百分位切割，然后再归一化\n",
    "            new_label_pro = self.cut_label_change(new_label_pro, per)\n",
    "            node_memory = self.label_frequency_normalize(new_label_pro)\n",
    "            accept_id = self.label_frequency_normalize(new_accept_id)\n",
    "        elif c_model == 2:\n",
    "            # （2）对所有标签归一化，再按照固定的阈值切割后再归一化\n",
    "            new_label_pro = self.label_frequency_normalize(new_label_pro)\n",
    "            node_memory = self.cut_label(new_label_pro,cut_threshold)\n",
    "            node_memory = self.label_frequency_normalize(node_memory)\n",
    "            accept_id = self.label_frequency_normalize(new_accept_id)\n",
    "        elif c_model == 3:\n",
    "            # 对节点和所有标签归一化，再按照固定的阈值切割\n",
    "            new_label_pro = self.label_frequency_normalize(new_label_pro)\n",
    "            node_memory = self.cut_label(new_label_pro,cut_threshold)\n",
    "            node_memory = self.label_frequency_normalize(node_memory)\n",
    "            new_accept_id = self.label_frequency_normalize(new_accept_id)\n",
    "            accept_id = self.cut_label(new_accept_id,cut_threshold)\n",
    "            accept_id = self.label_frequency_normalize(accept_id)\n",
    "        elif c_model == 4:\n",
    "            # 对节点和所有标签按照百分位切割，然后再归一化\n",
    "            new_label_pro = self.cut_label_change(new_label_pro, per)\n",
    "            new_accept_id = self.cut_label_change(new_accept_id, per)\n",
    "            node_memory = self.label_frequency_normalize(new_label_pro)\n",
    "            accept_id = self.label_frequency_normalize(new_accept_id)\n",
    "\n",
    "        return (node_memory, accept_id)\n",
    "\n",
    "    def make_filedir(self,filepath):\n",
    "        if not os.path.exists(filepath):\n",
    "            os.mkdir(filepath)\n",
    "        return filepath\n",
    "\n",
    "    def write_nodel_label(self,node_label,filepath,filename):\n",
    "        with open(filepath+filename+'.txt','w',encoding='utf-8')as f:\n",
    "            for id ,label_fre in node_label.items():\n",
    "                # print(id)\n",
    "                f.write(str(id)+' ')\n",
    "                for label ,fre in label_fre:\n",
    "                    f.write(str(label)+':'+str(fre)+' ')\n",
    "                f.write('\\n')\n",
    "        return\n",
    "\n",
    "    def write_dict_list(self,d_list,filename):\n",
    "        with open(filename,'w',encoding='utf-8')as f:\n",
    "            for k ,v in d_list.items():\n",
    "                f.write(str(k)+':'+' '.join(v))\n",
    "                f.write('\\n')\n",
    "        return\n",
    "\n",
    "    def stop_condition_stable(self,l,tag_com=None,stop_condition=1,stop_Threshold=0.0001):\n",
    "        #l 是循环次数\n",
    "        \"\"\"stop the iteration when satisfy the condition\n",
    "        'stop_condition' means the condition_form\n",
    "        'stop_condition': the labels remain stable   how to compare the previous label and current label ?\n",
    "        'stop_condition': the change of moudularity is lower than the threshold\n",
    "        'stop_condition': the change of label similarity is lower than the threshold\n",
    "        \"\"\"\n",
    "        if stop_condition == 0:\n",
    "            diff = 0\n",
    "            for node in tag_com.keys():\n",
    "                #完全相同还是仅仅标签相同？\n",
    "                if tag_com[l+1][node] != tag_com[l][node]:\n",
    "                    diff += 1\n",
    "            changeproportion = diff / self.node_count\n",
    "            if changeproportion < stop_Threshold:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        elif stop_condition == 1:\n",
    "            m=self.modu[l+1] - self.modu[l]\n",
    "            if abs(m) < stop_Threshold:\n",
    "                modu=sorted(self.modu.items() ,reverse=True,key= lambda k:k[1])\n",
    "                print(\"排序后的模块度值\")\n",
    "                print(modu)\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "\n",
    "    def label_propagation(self,filepath,G,neighbor,node_id_memory,node_label_memory,wordprocess,model,order_list,sim_threshold,run_mode=1,c_model=1,per=25,cut_threshold=0.05):\n",
    "        # order_list,节点排序,可根据需要改变\n",
    "        # print(\"开始迭代循环\")\n",
    "        # 迭代循环\n",
    "        self.node_count=len(order_list)\n",
    "        file_label = self.make_filedir(filepath + 'node_label/')\n",
    "        file_id = self.make_filedir(filepath + 'node_id/')\n",
    "        file_com = self.make_filedir(filepath + 'community/')\n",
    "        file_drop=self.make_filedir(filepath + 'drop_nest_community/')\n",
    "        self.node_memory[0] = node_label_memory\n",
    "        self.accept_id[0] = node_id_memory\n",
    "        for t in range(100):\n",
    "            # 异步更新\n",
    "            print('开始第{}次迭代'.format(t+1))\n",
    "\n",
    "            for listener in order_list:\n",
    "                nei=[]\n",
    "                w_listener=[]\n",
    "                w_speaker=[]\n",
    "                listener=str(listener)\n",
    "                # print(listener)\n",
    "                label_list = {}\n",
    "                id_list = {}\n",
    "                nei= neighbor[listener]\n",
    "                n_listener=node_label_memory[listener]\n",
    "                w_listener = [l for l,p in n_listener]\n",
    "                # print(node_label_memory[listener])\n",
    "                # print(node_label_memory)\n",
    "                c = 0\n",
    "                for speaker in nei:\n",
    "                    speaker=str(speaker)\n",
    "                    w_speaker=[]\n",
    "                    # print(speaker)\n",
    "\n",
    "                    nlm=node_label_memory[speaker]\n",
    "                    # print(nlm)\n",
    "                    w_speaker=[k for k ,v in nlm]\n",
    "                    # print(w_speaker)\n",
    "\n",
    "                    l_s = wordprocess.words_similarity(w_listener,w_speaker,model)\n",
    "\n",
    "                    if l_s >= sim_threshold:\n",
    "                        c+=1\n",
    "                        for slabel, prob in nlm:\n",
    "                            # add_pro = prob * l_s\n",
    "                            add_pro = prob\n",
    "                            if slabel in label_list:\n",
    "                                label_list[slabel] += add_pro\n",
    "                            else:\n",
    "                                label_list[slabel] = add_pro\n",
    "                        for id, fre in node_id_memory[speaker]:\n",
    "                            # add_fre = fre * l_s\n",
    "                            add_fre = fre\n",
    "                            if id in id_list:\n",
    "                                id_list[id] += add_fre\n",
    "                            else:\n",
    "                                id_list[id] = add_fre\n",
    "                    else:\n",
    "                            pass\n",
    "\n",
    "                if c==0:\n",
    "                    continue\n",
    "                else:\n",
    "                    label_list = sorted(label_list.items(), reverse=True, key=lambda k: k[1])\n",
    "                    id_list = sorted(id_list.items(), reverse=True, key=lambda k: k[1])\n",
    "                    #如果还出现重复，实际上是随机选择了\n",
    "                    l_max = label_list[0]\n",
    "                    i_max = id_list[0]\n",
    "                    #获取listener当前标签,注意dict之后会改变顺序\n",
    "                    listen_node_label=dict(n_listener)\n",
    "                    listen_id_label=dict(node_id_memory[listener])\n",
    "\n",
    "                    listen_node_label[l_max[0]]=listen_node_label.get(l_max[0],0)+l_max[1]\n",
    "                    listen_id_label[i_max[0]] = listen_id_label.get(i_max[0], 0) + i_max[1]\n",
    "\n",
    "                    node_label_memory[listener] = list(listen_node_label.items())\n",
    "                    node_id_memory[listener] = list(listen_id_label.items())\n",
    "\n",
    "            #如果需要截断，使用cut_model()\n",
    "            # node_label_memory=self.label_frequency_normalize(node_label_memory)\n",
    "            # node_id_memory=self.label_frequency_normalize(node_id_memory)\n",
    "            node_label_memory,node_id_memory=self.cut_model(node_label_memory,node_id_memory,c_model,per,cut_threshold)\n",
    "            self.node_memory[t + 1] = node_label_memory\n",
    "            self.accept_id[t+1]=node_id_memory\n",
    "\n",
    "            self.write_nodel_label(self.node_memory[t + 1],file_label, 'node_label_'+str(t + 1))\n",
    "            self.write_nodel_label(self.accept_id[t + 1],file_id, 'node_id_'+str(t + 1))\n",
    "            #partition  社团划分结果，NodeClustering类，comm  {com_id:[]}\n",
    "            #new_nodel_label  规定了节点所属的社团数后截断得到的node_label  {node_id:[(label,pro)]}\n",
    "            #comm_size   每个社团的节点个数，{com_id:node_count}\n",
    "            #node_com_size  每个节点的社团个数  {node_id:com_count}\n",
    "            if run_mode == 1:\n",
    "\n",
    "                # 以节点序号为社团标签\n",
    "                # #如果使用非重叠模块度，那get_community中的最后一个参数c必须选1\n",
    "                # partition, comm, new_node_label, comm_size, node_com_size = self.get_community(t+1,self.accept_id[t + 1], 1)\n",
    "                # self.modu[t + 1] = ev.newman_girvan_modularity(self.G,partition).score\n",
    "                # print(self.modu[t + 1])\n",
    "\n",
    "                # 如果使用重叠模块度,c='all'或者规定的节点最多社团数，这个时候需要配合cut_model使用\n",
    "\n",
    "                partition, comm, node_com_label,node_com_size = self.get_community(G,t + 1,node_id_memory,file_com, 'all')\n",
    "\n",
    "            else:\n",
    "                # 以节点标签为社团标签\n",
    "                # 如果使用非重叠模块度，那get_community中的最后一个参数c必须选1\n",
    "                # partition, comm, new_node_label, comm_size, node_com_size = self.get_community(t + 1, self.node_memory[t + 1], 1)\n",
    "                #\n",
    "                # self.modu[t + 1] = ev.newman_girvan_modularity(self.G, partition).score\n",
    "                # print(self.modu[t + 1])\n",
    "                # 如果使用重叠模块度,c='all'或者规定的节点最多社团数，这个时候需要配合cut_model使用\n",
    "\n",
    "                partition, comm, node_com_label, node_com_size = self.get_community(G,t + 1,node_label_memory,file_com, 'all')\n",
    "            partition, comm, nest_com, comm_size = self.drop_nested_community(G,file_drop,comm,t+1)\n",
    "            new_node_com_label, nest_node, node_com_size,node_comm = self.drop_nested_node_community(file_drop,nest_com, node_com_label,t+1)\n",
    "\n",
    "            modu = ev.link_modularity(G, partition).score\n",
    "            self.modu[t + 1] = modu\n",
    "            #每个社团的大小\n",
    "\n",
    "            c_s= len(comm_size.keys())\n",
    "            self.com_count[t + 1] =c_s\n",
    "            self.com_iter_nodecount[t + 1] = comm_size\n",
    "            self.node_iter_comcount[t + 1] = node_com_size\n",
    "            print(\"社团数量{}\".format(c_s))\n",
    "            print(\"模块度为{}\".format(modu))\n",
    "            if t>0:\n",
    "                #注意，如果需要第一种停止方式：节点的tag相同，则需要传入第二个参数给stop_condition_stable(t,tag_com)\n",
    "                if self.stop_condition_stable(t,tag_com=self.node_memory,stop_condition=1,stop_Threshold=0.001):\n",
    "                    print(\"The {} time loop end\".format(t+1))\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"The loop is terminated because the condition is satisfied or the loop  have get the max_iteration.the current loop time is {}\".format(t+1))\n",
    "                    break\n",
    "        # 以下的new_node_com_label 实际上是节点的社团归属，node_id或是label_id取决于run_model\n",
    "        #需要注意的是new_node_com_label 是没有归一化的，因为节点标签已经没有办法根据删去的社团找到，因此残留了标签？\n",
    "        #实际上这里是不必要的，需要取最后一次即可\n",
    "\n",
    "        com_label = self.get_community_label(file_com,comm, node_label_memory)\n",
    "        self.write_dict_list(comm,filepath+'final_community.txt')\n",
    "        self.write_dict_list(node_comm, filepath + 'final_node_to_community.txt')\n",
    "        readwrite.write_community_csv(partition,filepath+'cdlib_final_community.csv')\n",
    "        self.write_json_file(self.com_count, filepath + 'com_count.txt')\n",
    "        self.write_json_file(self.com_iter_nodecount, filepath + 'com_iter_nodecount.txt')\n",
    "        self.write_json_file(self.node_iter_comcount, filepath + 'node_iter_comcount.txt')\n",
    "        self.write_json_file(self.modu, filepath + 'modularity.txt')\n",
    "        self.write_json_file(self.accept_id, filepath + 'accept_id.txt')\n",
    "        self.write_json_file(self.node_memory, filepath + 'node_label.txt')\n",
    "        # self.write_json_file(self.node_similarity, self.fileout + 'node_similarity.txt')\n",
    "        print(\"删除的社团数量{},受到影响的节点数量{}\".format(len(nest_com), len(nest_node)))\n",
    "        print('最终的社团数量{}'.format(c_s))\n",
    "        print(\"最终的模块度{}\".format(modu))\n",
    "        conduance=ev.conductance(G,partition).score\n",
    "        print(\"电导率为{}\".format(conduance))\n",
    "\n",
    "        return partition, com_label,modu,conduance,c_s\n",
    "\n",
    "    def Emodularity(self, G, com_node, neigh, node_com):\n",
    "        #com_node: {com:[node_id]}, neigh:  {node_id:[node_id,node_id]}, node_com  {node_id:[com_id]}\n",
    "\n",
    "        com = com_node.keys()\n",
    "        m = float(G.number_of_edges())\n",
    "        score = 0\n",
    "        error_node = 0\n",
    "\n",
    "        for c in com:\n",
    "            for v in com_node[c]:\n",
    "                  for u in com_node[c]:\n",
    "                    if v == u:\n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            if u in neigh[v]:\n",
    "                                a = 1\n",
    "                            else:\n",
    "                                a = 0\n",
    "                            ov = len(node_com[v])\n",
    "                            ou = len(node_com[u])\n",
    "                            kv = len(neigh[v])\n",
    "                            ku = len(neigh[u])\n",
    "                            score = score + (1 / (ov * ou) * (a - (kv * ku) / (2 * m)))\n",
    "                        except Exception as e:\n",
    "                            print(\"节点{}错误\".format(v))\n",
    "                            error_node = error_node + 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            pass\n",
    "        print(\"错误节点数{}\".format(error_node))\n",
    "        score = float(score / (2 * m))\n",
    "        return score\n",
    "\n",
    "    def write_json_file(self,data,filename):\n",
    "        f=open(filename,'w',encoding='utf-8')\n",
    "        json.dump(data,f,ensure_ascii=False,indent=4)\n",
    "    def drop_nested_community(self,G,file,c,t):\n",
    "        communities=c.copy()\n",
    "        #communities   {com_id:[node_id,,]}\n",
    "        # Remove nested communities\n",
    "        nested_communities = set()\n",
    "        keys = list(communities.keys())\n",
    "        for i, label0 in enumerate(keys[:-1]):\n",
    "            comm0 = set(communities[label0])\n",
    "            for label1 in keys[i + 1:]:\n",
    "                comm1 = set(communities[label1])\n",
    "                if comm0.issubset(comm1):\n",
    "                    nested_communities.add(label0)\n",
    "                elif comm0.issuperset(comm1):\n",
    "                    nested_communities.add(label1)\n",
    "\n",
    "        for comm in nested_communities:\n",
    "            del communities[comm]\n",
    "        n_c=pd.Series(list(nested_communities))\n",
    "        n_c.to_csv(file+'nested_communities_{}.csv'.format(t),index=False)\n",
    "        com=list(communities.values())\n",
    "        partition=NodeClustering(com, G, \"SLPA\", method_parameters={\"T\": 100, \"r\": 0.25}, overlap=True)\n",
    "\n",
    "        self.write_dict_list(communities,file+'communities_node_{}.txt'.format(t))\n",
    "        com_size={com:len(node_list) for com ,node_list in communities.items()}\n",
    "        return partition,communities,nested_communities,com_size\n",
    "    def drop_nested_node_community(self,file,nested_com,n_label,t):\n",
    "        node_label=n_label.copy()\n",
    "        # node_label   {node_id:[(node_id,pro)]}\n",
    "        #nested_com    []需要删除的列表\n",
    "        com_node = defaultdict()\n",
    "        new_node_label={}\n",
    "        #统计受到影响的节点\n",
    "        nest_node={}\n",
    "        node_com_size={}\n",
    "        node_comm={}\n",
    "        for k, v in node_label.items():\n",
    "            label_pro=[]\n",
    "            l=[]\n",
    "            i=0\n",
    "            for label, pre in v:\n",
    "                if label in nested_com:\n",
    "                    i=i+1\n",
    "                    continue\n",
    "                else:\n",
    "                    label_pro.append((label,pre))\n",
    "                    l.append(label)\n",
    "            nest_node[k]=i\n",
    "            label_pro=sorted(label_pro,reverse=True,key= lambda k:k[1])\n",
    "            new_node_label[k]=label_pro\n",
    "            node_com_size[k]=len(label_pro)\n",
    "            node_comm[k]=l\n",
    "        # new_node_label=self.label_frequency_normalize(new_node_label)\n",
    "        n_cover = [k for k, v in node_label.items() if len(v) == 0]\n",
    "        print(\"无归属社区节点有{}个\".format(len(n_cover)))\n",
    "        n_cover=pd.Series(n_cover)\n",
    "        n_cover.to_csv(file+'no_community_node_{}.csv'.format(t),index=False)\n",
    "\n",
    "        n_n=pd.Series(list(nest_node))\n",
    "        n_n.to_csv(file+'nest_node.csv',index=True)\n",
    "        self.write_dict_list(node_comm,file+'node_communities_{}.txt'.format(t))\n",
    "        return new_node_label,nest_node,node_com_size,node_comm\n",
    "\n",
    "\n",
    "\n",
    "    def get_community(self,G,t,node_label,filepath,node_com_count='all'):\n",
    "        #node_label  {node_id:[(label,pro)]}\n",
    "        #输出{com_id:[node_id]}\n",
    "        comm=defaultdict()\n",
    "\n",
    "        node_com_size={}\n",
    "        new_node_label ={}\n",
    "        for id ,l_pro in node_label.items():\n",
    "            label_pro=sorted(l_pro,reverse=True,key= lambda k:k[1])\n",
    "            #确定节点所属社团个数\n",
    "            if node_com_count =='all':\n",
    "                pass\n",
    "            elif node_com_count==1:\n",
    "                label_pro=l_pro[0]\n",
    "            else:\n",
    "                label_pro=[(l,p) for l ,p in l_pro if p >node_com_count]\n",
    "            #计算节点所属社团个数\n",
    "            node_com_size[id]=len(label_pro)\n",
    "            #记录新的节点社团标签\n",
    "            new_node_label[id]=label_pro\n",
    "            #转化为{com:[node_id]}\n",
    "            for label ,pro in label_pro:\n",
    "                if label in comm.keys():\n",
    "                    comm[label]+=[id]\n",
    "                else:\n",
    "                    comm[label]=[id]\n",
    "\n",
    "        self.write_dict_list(comm,filepath+'{}iter_community.txt'.format(t))\n",
    "        comm_list = list(comm.values())\n",
    "        #注意返回的是cdlib中的类，方便下一步直接调用\n",
    "        communities = NodeClustering(comm_list, G, \"SLPA\", method_parameters={\"T\": 100, \"r\": 0.25}, overlap=True)\n",
    "        # readwrite.write_community_csv(communities, fileout + '{}_gold_community.csv'.format(g_name))\n",
    "        return (communities,comm,new_node_label,node_com_size)\n",
    "    def get_community_label(self,file,comm,node_label):\n",
    "        #获取社团标签\n",
    "        #com_label  {com:{label:fre}}\n",
    "        com_label={}\n",
    "        # print(comm)\n",
    "        for com ,node in comm.items():\n",
    "            #计数\n",
    "            label_list=[]\n",
    "            for n in node:\n",
    "                l_list=[l for l ,p in node_label[n]]\n",
    "                label_list.extend(l_list)\n",
    "            com_label[com]=dict(Counter(label_list))\n",
    "        self.write_json_file(com_label,file+'com_label.txt')\n",
    "        return com_label\n",
    "    def label_wordcloud(self,fileout,com_lable,image_file):\n",
    "        #com_label  {com:{label:fre}}\n",
    "\n",
    "        for com ,label in com_lable.items():\n",
    "            # print(label)\n",
    "\n",
    "            # read the mask image\n",
    "            # taken from\n",
    "            # http://www.stencilry.org/stencils/movies/alice%20in%20wonderland/255fk.jpg\n",
    "            alice_mask = np.array(Image.open(image_file))\n",
    "            stopwords = set(STOPWORDS)\n",
    "            # stopwords.add(\"said\")\n",
    "            wc = WordCloud(background_color=\"white\", max_words=2000, mask=alice_mask,\n",
    "                           stopwords=stopwords, contour_width=3, contour_color='steelblue')\n",
    "            # generate word cloud\n",
    "            wc.generate_from_frequencies(label)\n",
    "\n",
    "            # store to file\n",
    "            wc.to_file(fileout+'{}_wordcloud.png'.format(com))\n",
    "            # show\n",
    "            plt.imshow(wc, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.figure()\n",
    "            plt.imshow(alice_mask, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "    def Plot(self,file,data,title,xlabel,ylabel):\n",
    "        s=plt.figure(figsize=(10,10),dpi=500)\n",
    "        x=list(data.keys())\n",
    "        y=list(data.values())\n",
    "        plt.plot(x,y)\n",
    "        plt.title(title)\n",
    "        plt.xlim(0, len(x) + 1)\n",
    "        plt.ylim(0, max(y) + min(y))\n",
    "        plt.xticks(np.linspace(0, len(x), num=10, endpoint=True))\n",
    "        plt.yticks(np.linspace(0, max(y), num=10, endpoint=True))\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.savefig(file+title+'.png',dpi=500)\n",
    "        plt.show()\n",
    "\n",
    "def make_filedir(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.mkdir(filepath)\n",
    "    return filepath\n",
    "def slpa_main(graph_name):\n",
    "    # 初始化输入信息\n",
    "\n",
    "    # 输入文件\n",
    "    # filein = 'D:/Backup Plus/slpa-dataset/slpa_compare_input/'\n",
    "    filein='F:/aps_out/aps/APS_network/aps_article_network/article_journal_network/'\n",
    "    filein1= 'D:/Backup Plus/slpa-dataset/slpa_compare_input/'\n",
    "    # Word2vec模型文件\n",
    "    model_file = filein1 + 'GoogleNews-vectors-negative300.bin'\n",
    "    # 词云背景图片\n",
    "    image_file = filein + 'logo.png'\n",
    "    # 文件\n",
    "    # graph_name = 'graph_pub_all_asyn_simcut3_simpre/'\n",
    "    file_out = make_filedir('F:/slpa_aps_cut2pre/' + graph_name)\n",
    "    file_graph = make_filedir(file_out + '/graph_process/')\n",
    "    file_word = make_filedir(file_out + '/word_process/')\n",
    "    # 图边信息,需要和下面的节点内容文件对应\n",
    "    G_file = filein + graph_name+'.gexf'\n",
    "    # 节点内容文件\n",
    "    Idcontent_file = filein +graph_name+ '.txt'\n",
    "\n",
    "    # 设定算法阈值\n",
    "    # 最大循环次数，默认100\n",
    "    # 节点相似度阈值\n",
    "    Sim_Thresholds = [0, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "    # Sim_Thresholds = [0.1, 0.2]\n",
    "    # 停止条件,0:节点标签不再变动；1：模块度变化的绝对值小于阈值；2：未实现\n",
    "    stop_conditions = [0, 1, 2]\n",
    "    # stop_Thresholds = [0.0001, 0.01]\n",
    "    # 邻居节点范围，0： 0-1 hop;1；1 hop,2 :2 hop\n",
    "    hops = [0, 1, 2]\n",
    "    stop_Threshold = [0.0001, 0.001]\n",
    "    cut_threshold = [0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4,\n",
    "                     0.45, 0.5]\n",
    "\n",
    "    # 初始化类\n",
    "    g = GraphProcess(filein, file_graph)\n",
    "    wordprocess = WordProcess(filein, file_word, model_file)\n",
    "    # 加载模型\n",
    "    model = wordprocess.load_word2vec()\n",
    "    print(\"处理图{}\".format(graph_name))\n",
    "    # 读取图\n",
    "    G = nx.read_gexf(G_file, node_type=str)\n",
    "    # 判断图的类型\n",
    "    if nx.is_directed(G):\n",
    "        G = G.to_undirected()\n",
    "\n",
    "    # 去除度为零的节点\n",
    "    c_G, skip_node = g.isloated(G)\n",
    "    # 若需要可去除度小于阈值的节点\n",
    "    # c_G, skip_node = g.prune(G)\n",
    "    g.visual_G(G, c_G)\n",
    "    # 获取邻居节点,hop=0:0-1 hop ;1: 1 hop;2: 2-hop\n",
    "    neighbors = g.neighbor_hop(c_G, 1)\n",
    "    # compare_algorithm_crisp(c_G,filein+'crisp.xlsx')\n",
    "    # compare_algorithm_overlap(c_G,filein+'overlap.xlsx')\n",
    "    # c_G,skip_node=g.prune(G,limit=1)\n",
    "    # 初始化节点更新类\n",
    "    o = NodeOrder(c_G)\n",
    "    # 节点排序,返回排序结果和同一指标值的统计\n",
    "    # order_list,c=o.degree_order()\n",
    "    # o1 = pd.Series(order_list)\n",
    "    # print(\"保存节点排序结果\")\n",
    "    # o1.to_csv(file_graph + 'order_list_.csv', index=False)\n",
    "    # c1=pd.Series(c)\n",
    "    # c1.to_excel(file_graph+'order_count_.xlsx',index=True)\n",
    "\n",
    "    # 初始化slpa_extend\n",
    "    file_slpa = make_filedir(file_out + '/slpa_perform/')\n",
    "    slpa = SlpaExtend(filein, file_slpa)\n",
    "    # 初始化标签\n",
    "    # #获取节点内容\n",
    "    node_content = slpa.get_node_content(Idcontent_file)\n",
    "    p_node_content = slpa.prune_node_content(skip_node, node_content)\n",
    "    print(\"原始节点个数为{},保存节点个数为{},删去节点{}个\".format(len(node_content), len(p_node_content),\n",
    "                                               len(p_node_content) - len(node_content)))\n",
    "\n",
    "    node_label, node_id = slpa.initialize_label(wordprocess, p_node_content)\n",
    "    # 循环次数\n",
    "    iter_all = 3\n",
    "    iter_cal = {}\n",
    "    file_plot = make_filedir(file_slpa + 'plot/')\n",
    "    for v in Sim_Thresholds:\n",
    "        for c in cut_threshold:\n",
    "\n",
    "            for t in range(iter_all):\n",
    "                cal = {}\n",
    "                f_n = str(v) + '_' + str(c) + '_' + str(t)\n",
    "                file_propagation = make_filedir(file_slpa + f_n + '/')\n",
    "                # 随机节点排序时\n",
    "                order_list = o.random_order()\n",
    "                o1 = pd.Series(order_list)\n",
    "                print(\"保存节点排序结果\")\n",
    "                o1.to_csv(file_graph + 'order_list_{}.csv'.format(t), index=False)\n",
    "                print(\"SLPA算法第{}次循环\".format(t))\n",
    "                # partition ：cdlib的nodeclustering ,com_label:社团  {com:[node]};modu:模块度\n",
    "                # 注意变量在此处赋值给某一个参数,由于选了cut 模式，因此c_model只能选择2或者3\n",
    "                node_id_memory = node_id.copy()\n",
    "                node_label_memory = node_label.copy()\n",
    "                st = time.time()\n",
    "                partition, com_lable, modu, conduance, c_s = slpa.label_propagation(file_propagation, c_G, neighbors,\n",
    "                                                                                    node_id_memory, node_label_memory,\n",
    "                                                                                    wordprocess, model, order_list,\n",
    "                                                                                    sim_threshold=v, run_mode=1,\n",
    "                                                                                    c_model=2,\n",
    "                                                                                    per=25, cut_threshold=c)\n",
    "                # readwrite.write_community_csv(partition, file_slpa + 'cdlib_final_community.csv')\n",
    "                sp = time.time()\n",
    "                cal['modularity'] = modu\n",
    "                cal['conduance'] = conduance\n",
    "                cal['comcount'] = c_s\n",
    "                cal['time'] = sp - st\n",
    "                slpa.Plot(file_plot, slpa.modu, \"modularity_{}\".format(f_n), xlabel='iter', ylabel='modularuty')\n",
    "                slpa.Plot(file_plot, slpa.com_count, 'community_count_{}'.format(f_n), 'iter', 'community size')\n",
    "                try:\n",
    "                    cal['newman_modularity'] = ev.newman_girvan_modularity(c_G, partition).score\n",
    "                except:\n",
    "                    print(\"newman modularity failed\")\n",
    "\n",
    "                cal['conduance'] = ev.conductance(c_G, partition).score\n",
    "                iter_cal[f_n] = cal\n",
    "                # 避免绘图后出现图的节点改变的问题\n",
    "                # G_p = c_G.copy()\n",
    "                # plt.figure()\n",
    "                # viz.plot_network_clusters(G_p, partition, position=nx.spring_layout(c_G), figsize=(10, 10),\n",
    "                #                           plot_overlaps=True)\n",
    "                # plt.savefig(file_plot + \"network_clusters_{}.png\".format(f_n), dpi=500)\n",
    "                #\n",
    "                # plt.figure()\n",
    "                # viz.plot_community_graph(G_p, partition, figsize=(10, 10), plot_overlaps=True)\n",
    "                # plt.savefig(file_plot + \"network_community_{}.png\".format(f_n), dpi=500)\n",
    "                # # plt.savefig(file_slpa + \"network_community.eps\", format='eps',dpi=500)\n",
    "                # file_wordcloud=make_filedir(file_propagation+'wordcloud/')\n",
    "                # slpa.label_wordcloud(file_wordcloud,com_lable, image_file)\n",
    "                # plt.show()\n",
    "                # 显式置空类的属性\n",
    "                slpa.accept_id = {}\n",
    "                slpa.node_memory = {}\n",
    "                slpa.modu = {}\n",
    "                slpa.com_count = {}\n",
    "                slpa.com_iter_nodecount = {}\n",
    "                slpa.node_iter_comcount = {}\n",
    "\n",
    "    dt = pd.DataFrame(iter_cal)\n",
    "    dt1 = dt.stack()\n",
    "    dt2 = dt1.unstack(0)\n",
    "    dt2.to_excel(file_slpa + 'calculate_output.xlsx', index=True, header=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #'graph-16', 'graph-24','graph-75','graph_pub_all' ,\n",
    "    # g_file1 = ['graph-107', 'graph-131', 'graph-144', 'graph-145', 'graph-162', 'graph-182', 'graph-199' ]\n",
    "    # 'PRI',\n",
    "    g_file1 = [ 'PRSTPER','RMP']\n",
    "    for graph_name in g_file1:\n",
    "        slpa_main(graph_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
