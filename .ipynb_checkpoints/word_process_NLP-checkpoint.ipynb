{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordProcess():\n",
    "    \"\"\"函数列表：获取文件列表get_filename、获取文件内容get_content、获取wiki语料\"\"\"\n",
    "    #数据预处理部分\n",
    "    def __init__(self,filein,fileout,model_file=None):\n",
    "        self.filein=filein\n",
    "        self.fileout=fileout\n",
    "        self.stoplist = []\n",
    "        self.model_file=model_file\n",
    "    def make_path(self,filepath):\n",
    "        if not os.path.exists(filepath):\n",
    "            os.mkdir(filepath)\n",
    "        return filepath\n",
    "\n",
    "\n",
    "    def get_filename(self,fileDir):\n",
    "        \"\"\"获取文件夹下的文件路径和名称，仅一层\"\"\"\n",
    "        filepath=[]\n",
    "        if os.path.exists(fileDir):\n",
    "            for filename in os.listdir(fileDir):\n",
    "                filepath.append(os.path.join(fileDir,filename))\n",
    "        return filepath\n",
    "    def get_filecontent(self,filename):\n",
    "        \"\"\"文本内容转化为列表，一行一列\"\"\"\n",
    "        #to [[],[]]\n",
    "        data=[]\n",
    "        with open(filename,'r',encoding='utf-8')as f:\n",
    "            for line in f.readlines():\n",
    "                data.append(line.strip(' ').strip('\\n').strip('\\t'))\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "    def merge_corpus(self,infilepath,output_file):\n",
    "        \"\"\"\"将多个text文件合并为一个并输出,每个文本为一个单元\"\"\"\n",
    "        data=[]\n",
    "        with open(self.fileout+output_file+'/.txt','w',encoding='utf-8')as f:\n",
    "            for filename in os.listdir(infilepath):\n",
    "                if '.txt' in filename:\n",
    "                    file_path = os.path.join(infilepath, filename)\n",
    "                    t = []\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\")as fr:\n",
    "\n",
    "                        for line in fr.readline():\n",
    "                            line = line.strip('\\n').strip('\\t')\n",
    "                            if len(line) > 0:\n",
    "                                t.append(line)\n",
    "\n",
    "                    s=' '.join(list(map(str,t)))\n",
    "                    f.writelines(s+'\\n')\n",
    "                    data.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "        return data\n",
    "    def write_matrix(self,filename,text):\n",
    "        #text=[]\n",
    "        #写入矩阵 [[],[]]\n",
    "\n",
    "        with open(filename,'w',encoding='utf-8')as f:\n",
    "            for l in text:\n",
    "                f.write(' '.join(l))\n",
    "                f.write('\\n')\n",
    "        return\n",
    "    def write_content(self,filename,text):\n",
    "        \"\"\"[[(),()],[(),()]]\"\"\"\n",
    "        with open(filename,'w',encoding='utf-8')as f:\n",
    "            for e in text:\n",
    "                for e1,e2 in e:\n",
    "                    f.write(str(e1)+':'+str(e2)+' ')\n",
    "                f.write('\\n')\n",
    "\n",
    "    def record_error(self, filename, errorcontent):\n",
    "        with open(filename, 'a', encoding='utf-8')as f:\n",
    "            f.write(errorcontent + '\\n')\n",
    "\n",
    "     #将下载下来的维基百科xml文件装化为txt文件\n",
    "    def parse_wiki_corpora(self,name):\n",
    "        wname=self.filein+name\n",
    "        with open(self.fileout+'wiki_text','w',encoding='utf-8')as f:\n",
    "            wiki = WikiCorpus(wname, lemmatize=False, dictionary={})\n",
    "            for text in wiki.get_texts():\n",
    "                f.write(' '.join(text) + \"\\n\")\n",
    "        return\n",
    "\n",
    "    #中文分词部分\n",
    "    #jieba分词\n",
    "    def add_dict_jieba(self,user_dict):\n",
    "        with open(self.fileout+user_dict,'r',encoding='utf-8')as f:\n",
    "            word=f.readline()\n",
    "            jieba.add_word(word.strip().strip('\\n').strip('\\t'))\n",
    "    def add_jieba_word(self,word,fre):\n",
    "        jieba.add_word(word,freq=fre)\n",
    "\n",
    "    def cfenci(self,doc,name):\n",
    "        \"\"\"输入每行是一个文本\"\"\"\n",
    "        new_doc=[]#[['',''],['','']]\n",
    "        #https://zhuanlan.zhihu.com/p/53521380\n",
    "        fileout=self.fileout+\"cfenci/\"\n",
    "        self.make_path(fileout)\n",
    "        with open(fileout+name+'.txt','w',encoding='utf-8')as f:\n",
    "            for line in doc:\n",
    "                word=[]\n",
    "                words = jieba.cut(line, cut_all=False)\n",
    "                for w in words:\n",
    "                    if w not in self.stoplist:\n",
    "                        word.append(w)\n",
    "                new_doc.append(w)\n",
    "                f.write(' '.join(word))\n",
    "                f.write('\\n')\n",
    "        print(\"{}文档分词完毕\".format(name))\n",
    "        return new_doc\n",
    "\n",
    "    def stopwords(self,stop_name):\n",
    "        stoplist=[]\n",
    "        with open(self.filein+stop_name,'r',encoding='utf-8')as f:\n",
    "            for s in f.readlines():\n",
    "                s=s.strip().strip('\\n').strip('\\t')\n",
    "                stoplist.append(s)\n",
    "            self.stoplist=stoplist\n",
    "        return self.stoplist\n",
    "\n",
    "    #繁转简体,去除无关内容\n",
    "    def get_content_convert(self,doc):\n",
    "        \"\"\"Conversions 转换参数\n",
    "        hk2s: Traditional Chinese (Hong Kong standard) to Simplified Chinese\n",
    "        s2hk: Simplified Chinese to Traditional Chinese (Hong Kong standard)\n",
    "        s2t: Simplified Chinese to Traditional Chinese\n",
    "        s2tw: Simplified Chinese to Traditional Chinese (Taiwan standard)\n",
    "        s2twp: Simplified Chinese to Traditional Chinese (Taiwan standard, with phrases)\n",
    "        t2hk: Traditional Chinese to Traditional Chinese (Hong Kong standard)\n",
    "        t2s: Traditional Chinese to Simplified Chinese\n",
    "        t2tw: Traditional Chinese to Traditional Chinese (Taiwan standard)\n",
    "        tw2s: Traditional Chinese (Taiwan standard) to Simplified Chinese\n",
    "        tw2sp: Traditional Chinese (Taiwan standard) to Simplified Chinese (with phrases)\"\"\"\n",
    "        text=[]\n",
    "        #regex_str=re.compile(\"[^<doc.*>$]|[^</doc>$]\")\n",
    "        cc = opencc.OpenCC('t2s')\n",
    "        with open(self.fileout+'opencc_convert.txt','w',encoding='utf-8')as f:\n",
    "            for c in doc:\n",
    "                c=c.strip('\\n').strip('\\t')\n",
    "                # 移除空行，移除其他字符\n",
    "                #regex_str.sub('',c)\n",
    "                if len(c)>0:\n",
    "                    c=cc.convert(c)\n",
    "                    text.append(c)\n",
    "                    f.write(c+'\\n')\n",
    "        return text\n",
    "    #英文分词\n",
    "\n",
    "    def word_cut(self,doc,name='efenci.txt'):\n",
    "        # stemmerlan = LancasterStemmer()\n",
    "        # stemmerporter = PorterStemmer()\n",
    "        # lemmatizer = WordNetLemmatizer()\n",
    "        stop_list=stopwords.words('english')\n",
    "        #去除标点符号后分词和不去除标点符号分词有区别吗？应该是没有\n",
    "        x = re.compile('[%s]' % re.escape(string.punctuation))  #去除标点符号\n",
    "\n",
    "        text=[]\n",
    "        for d in doc :\n",
    "            #统一为小写.去除标点符号\n",
    "            d= x.sub(u'', d.lower())\n",
    "            #避免有中文符号的影响 尤其是“”‘’\n",
    "            d = re.sub(\"[{}]+\".format(punctuation), \"\", d)\n",
    "            if d is not None:\n",
    "                w = self.lemmatize_all(d)\n",
    "                # 处理词形\n",
    "                word = [s for s in w if s not in stop_list]\n",
    "                # 处理词干，提取词干可能会有影响，慎重使用,如 apples 会被处理成appl\n",
    "                # word = [stemmerlan.stem(s) for s in word]\n",
    "                text.append(word)\n",
    "            else:\n",
    "                pass\n",
    "        self.write_matrix(self.fileout+name,text)\n",
    "        return text\n",
    "    #提取词干\n",
    "    def get_stem(self,doc):\n",
    "        stemerporter=PorterStemmer()\n",
    "        new_word=[]\n",
    "        for d in doc:\n",
    "            w=stemerporter.stem(d)\n",
    "            new_word.append(w)\n",
    "        return new_word\n",
    "    #词形\n",
    "    def lemmatize_all(self,sentence):\n",
    "        wnl = WordNetLemmatizer()\n",
    "        for word, tag in nltk.pos_tag(nltk.word_tokenize(sentence)):\n",
    "            if tag.startswith('NN'):\n",
    "                yield wnl.lemmatize(word, pos='n')\n",
    "            elif tag.startswith('VB'):\n",
    "                yield wnl.lemmatize(word, pos='v')\n",
    "            elif tag.startswith('JJ'):\n",
    "                yield wnl.lemmatize(word, pos='a')\n",
    "            elif tag.startswith('R'):\n",
    "                yield wnl.lemmatize(word, pos='r')\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def sentence_cut(self,doc):\n",
    "        #划分句子\n",
    "        text=sent_tokenize(doc)\n",
    "        return text\n",
    "    #向量化\n",
    "\n",
    "    def word_bag(self,texts,fre=0):\n",
    "        \"\"\"texts格式\n",
    "        [['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
    "        ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
    "        \"\"\"\n",
    "        # 去除低频词\n",
    "        frequency = defaultdict(int)\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                frequency[token] += 1\n",
    "        texts = [[token for token in text if frequency[token] > fre] for text in texts]\n",
    "        #输出词频\n",
    "        frequency=sorted(frequency.items(),reverse=True,key=lambda k:k[1])\n",
    "        with open(self.fileout+'all_word_frequence.txt','w',encoding='utf-8')as f:\n",
    "            json.dump(frequency,f,ensure_ascii=False,indent=4)\n",
    "        # bag-of-words\n",
    "        #pprint(texts[0:2])\n",
    "        dictionary = corpora.Dictionary(texts)\n",
    "        dictionary.save(self.fileout+'worddictionary.dict')#store the dictonary ,for future reference\n",
    "        #输出词典词\n",
    "        # pprint(dictionary.token2id)\n",
    "        # 将其他文档转化为向量,如果新的文档中出现了不在词袋中的词，那么这些词将被舍弃\n",
    "        # new_doc = 'Human computer interaction'\n",
    "        # new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "        # print(new_vec)\n",
    "        # 将原始的文档向量化\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        #pprint(corpus)\n",
    "        # 保存向量化后的文档\n",
    "        corpora.MmCorpus.serialize(self.fileout+'corpus.mm', corpus)\n",
    "        return (corpus,dictionary)\n",
    "\n",
    "    def read_content(self,dict_file,corpus_file):\n",
    "        print(\"读取词典数据和向量数据\")\n",
    "        # 词典数据\n",
    "        dictionary = corpora.Dictionary.load(dict_file)\n",
    "        # 向量数据\n",
    "        corpus = corpora.MmCorpus(corpus_file)\n",
    "        return (dictionary,corpus)\n",
    "\n",
    "    def word_tfidf(self,corpus):\n",
    "        tfidf = models.TfidfModel(corpus,normalize=True)  # initialize a model\n",
    "        # transform new document,use the model to transform vectors\n",
    "        # apply the model to a whole corpus,in this case,just use the same corpus ,\n",
    "        # Once the transformation model has been initialized, it can be used on any vectors\n",
    "        # (provided they come from the same vector space, of course), even if they were not used in the training corpus at all\n",
    "        # Calling model[corpus] only creates a wrapper around the old corpus document stream – actual conversions are done on-the-fly\n",
    "\n",
    "        corpus_tfidf = tfidf[corpus]\n",
    "        corpus_tfidf.save(self.fileout+'model.tfidf')\n",
    "        # when this done ,can save the results,see 语料库的保存\n",
    "        # We cannot convert the entire corpus at the time of calling corpus_transformed = model[corpus],\n",
    "        #for doc in corpus_tfidf:\n",
    "            #print(doc)\n",
    "        return corpus_tfidf\n",
    "    def word_lsi(self,curpos_tfidf,dictionary,num_topic):\n",
    "        # Transformations can also be serialized, one on top of another, in a sort of chain\n",
    "        # Here we transformed our Tf-Idf corpus via Latent Semantic Indexing into a latent 2-D space (2-D because we set num_topics=2)\n",
    "        lsi_model = models.LsiModel(curpos_tfidf, id2word=dictionary, num_topics=num_topic)\n",
    "        lsi_model.save(self.fileout+'model.lsi')\n",
    "        return lsi_model\n",
    "    def word_lda(self,corpus,dictionary):\n",
    "        lda_model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)\n",
    "        lda_model.save(self.fileout+'model.lda')\n",
    "        return lda_model\n",
    "    def train_word2vec_single(self,input_file,size=256, window=10, min_count=5):\n",
    "        #一行为一个文本,可以直接为txt文件名\n",
    "        sentence=LineSentence(input_file)\n",
    "        #防止语料过大无法加载\n",
    "        # 训练模型\n",
    "        model=Word2Vec(sentence,size=size,window=window,min_count=min_count,workers=multiprocessing.cpu_count(),iter=10)\n",
    "        model.save(self.fileout+'wiki_corpus.model')\n",
    "        model.wv.save_word2vec_format(self.fileout+'word2vec_format',binary=False)\n",
    "        return model\n",
    "    def train_word2vec_dir(self,input_dir,size=256, window=10, min_count=5):\n",
    "\n",
    "\n",
    "       # 输入语料目录:PathLineSentences(input_dir)，input_dir为语料目录，而不是文件名\n",
    "       #embedding size:256 共现窗口大小:10 去除出现次数5以下的词,多线程运行,迭代10次\n",
    "        model = Word2Vec(PathLineSentences(input_dir), size=size, window=window, min_count=min_count, workers=multiprocessing.cpu_count(), iter=10)\n",
    "\n",
    "        model.save(self.fileout + 'wiki_corpus.model')\n",
    "        model.wv.save_word2vec_format(self.fileout + 'word2vec_format', binary=False)\n",
    "        return model\n",
    "    # 一次性加载语料\n",
    "    def train_word2vec_filter(self):\n",
    "        return\n",
    "    def get_word_word2vector(self,words,model,k=300):\n",
    "        #需确保words是不重复的\n",
    "        word_vector=defaultdict()\n",
    "        i=0\n",
    "        for w in words :\n",
    "            if w in model.wv.vocab:\n",
    "                word_vector[w]=model.wv[w]\n",
    "            else:\n",
    "                i+=1\n",
    "                word_vector[w]=np.random.uniform(-0.25,0.25,k)\n",
    "        print(\"未登录词数为{}\".format(i))\n",
    "        return word_vector\n",
    "    \"\"\"关于归一化：\n",
    "       因为余弦值的范围是 [-1,+1] ，相似度计算时一般需要把值归一化到 [0,1]，一般通过如下方式：\n",
    "       sim = 0.5 + 0.5 * cosθ\n",
    "       若在欧氏距离公式中，取值范围会很大，一般通过如下方式归一化：\n",
    "       sim = 1 / (1 + dist(X,Y))\"\"\"\n",
    "    def vector_cosine(self,v1,v2):\n",
    "        d1=np.asarray([v1])\n",
    "        d2=np.asarray([v2])\n",
    "        num=float(d1.dot(d2.T))\n",
    "        denom=np.linalg.norm(d1)*np.linalg.norm(d2)\n",
    "        cos=num/denom #余弦值\n",
    "        sim=0.5+0.5*cos#根据皮尔逊相关系数归一化\n",
    "        return sim\n",
    "    def vector_euclidean(self,v1,v2):\n",
    "        d1 = np.asarray([v1])\n",
    "        d2 = np.asarray([v2])\n",
    "        dist = np.linalg.norm(d1 - d2)\n",
    "        sim = 1.0 / (1.0 + dist)  # 归一化\n",
    "        return sim\n",
    "\n",
    "\n",
    "\n",
    "    def get_text_bagvector(self,dictionary,curpus):\n",
    "        \"\"\"取出每个句子的词\",not for word2vec\n",
    "        curpus格式\n",
    "        [[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
    "        [(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555)]]\n",
    "        text格式：每行为词和词的频率\n",
    "        [[(),(),()],[(),()]]\"\"\"\n",
    "        text=[]\n",
    "        dict=dictionary.token2id\n",
    "        #dict格式：dict of(str,int)\n",
    "        for d in curpus:\n",
    "            doc=[]\n",
    "            for w ,f in d:\n",
    "                # 比较id，获得词的频率\n",
    "                t=[k for (k,v) in dict.items() if v==w][0]\n",
    "                doc.append((t,f))\n",
    "            doc.sort(key=lambda x: x[1], reverse=True)  # 进行排序\n",
    "            \"\"\"\"\n",
    "            #取出规定长度的词\n",
    "            if len(doc)<list_length:\n",
    "                pass\n",
    "            else:\n",
    "                doc=doc[0:list_length]\"\"\"\n",
    "            text.append(doc)\n",
    "        self.write_content(self.fileout+'word_frequence.txt',text)\n",
    "        return text\n",
    "\n",
    "    def load_word2vec(self):\n",
    "        #加载预训练模型，如果模型中没有这个词，应当如何处理?\n",
    "        # model = models.Word2Vec.load(self.model_file)\n",
    "        model_file=self.model_file\n",
    "        model=models.KeyedVectors.load_word2vec_format(model_file,binary=True)\n",
    "        return model\n",
    "    def wsimilarity_top_word(self,model,word,top=10):\n",
    "        #获取最相似的词\n",
    "        sim_word=model.most_similar(word,topn=top)\n",
    "        return sim_word\n",
    "    def words_similarity(self,w1,w2,model):\n",
    "\n",
    "        w1_new=[w for w in w1 if w in model.wv.vocab]\n",
    "        w2_new=[w for w in w2 if w in model.wv.vocab]\n",
    "        if len(w1)!=len(w1_new) or len(w2)!=len(w2_new):\n",
    "            self.record_error(self.fileout+'not_in_word2vec_vocab.txt',\"原句1：{},新句1:{};原句2:{},新句2{}\".format(w1,w1_new,w2,w2_new))\n",
    "        #比较两个词列表的相似性\n",
    "        if len(w1_new)!=0 and len(w2_new)!=0:\n",
    "            sim=model.wv.n_similarity(w1_new,w2_new)\n",
    "        else:\n",
    "            print(w1,w2)\n",
    "            sim=0\n",
    "        return sim\n",
    "\n",
    "    def compare_word_similarity(self,model,word_list):\n",
    "        word_pair_sim=[]\n",
    "        with open(self.fileout+\"word_pair_similarity.txt\",'a',encoding='utf-8')as f:\n",
    "            for b_word, e_word in word_list:\n",
    "                res = model.similarity(b_word, e_word)\n",
    "                w_p_s=set(b_word, e_word, res)\n",
    "                word_pair_sim.append(w_p_s)\n",
    "                f.write(w_p_s)\n",
    "                f.write('\\n')\n",
    "        return word_pair_sim\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
