{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SLPA\n",
    "from cdlib import algorithms,evaluation as ev,readwrite\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "def compare_slpa(g,f):\n",
    "    e=Emodularity()\n",
    "    r_l=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "    m_score=[]\n",
    "    for i in r_l:\n",
    "        i_j={}\n",
    "        for j in range(3):\n",
    "            c={}\n",
    "            st=time.time()\n",
    "            c_cdlib=algorithms.slpa(g,t=30,r=i)\n",
    "            sp=time.time()\n",
    "            readwrite.write_community_csv(c_cdlib,f+str(i)+'_'+str(j)+'.csv')\n",
    "            m_cdlib=ev.link_modularity(g,c_cdlib).score\n",
    "            p=ev.conductance(g,c_cdlib).score\n",
    "            o=e.emodularity(g,c_cdlib)\n",
    "            print(p)\n",
    "            print(m_cdlib)\n",
    "            c['link_modularity']=m_cdlib\n",
    "            c['conduance']=p\n",
    "            c['over_modularity']=o\n",
    "            c['time']=sp-st\n",
    "            c['com_count']=len(c_cdlib.communities)\n",
    "            c['iter']=j\n",
    "            i_j[j]=c\n",
    "        dt=pd.DataFrame(i_j)\n",
    "        dt1 = dt.stack()\n",
    "        dt2 = dt1.unstack(0)\n",
    "        dt2['r']=[i]*len(dt2.index)\n",
    "        dt2.to_excel(f + 'calculate_output_{}.xlsx'.format(i), index=True, header=True)\n",
    "        m_score.append(dt2)\n",
    "    m_score=pd.concat(m_score)\n",
    "    m_score.to_excel(f+'all_cal.xlsx')\n",
    "def make_filedir(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.mkdir(filepath)\n",
    "    return filepath\n",
    "fo='F:/aps_analysis/aps_compare_slpa/'\n",
    "fi='F:/aps_out/aps/APS_network/aps_article_network/article_journal_network/'\n",
    "# g_list=['PRSTPER','PRI','PRMATERIALS','RMP','PRAPPLIED','PRX','PRSTAB','PRPER','PRFLUIDS','PRAB','PRE','PR',]\n",
    "# ,'PRC','PRA','PRL','PRD'\n",
    "g_list = ['PRB']\n",
    "for g in g_list:\n",
    "    f=make_filedir(fo+g+'/')\n",
    "    G_file=fi+g+'.gexf'\n",
    "    G = nx.read_gexf(G_file, node_type=str)\n",
    "    # 判断图的类型\n",
    "    if nx.is_directed(G):\n",
    "        G = G.to_undirected()\n",
    "    s=list(nx.isolates(G))\n",
    "    G.remove_nodes_from(s)\n",
    "    compare_slpa(G,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEMO\n",
    "#比较demo\n",
    "from cdlib import algorithms,evaluation as ev,readwrite\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "class Emodularity():\n",
    "    def emodularity(self, G, partition):\n",
    "        #com_node: {com:[node_id]}, neigh:  {node_id:[node_id,node_id]}, node_com  {node_id:[com_id]}\n",
    "        comm=partition.communities\n",
    "        com_node=self.get_com_node(comm)\n",
    "        node_com=self.com_node_reversal(com_node)\n",
    "        neigh=self.Nodeneigh(G)\n",
    "        com = com_node.keys()\n",
    "        m = float(G.number_of_edges())\n",
    "        score = 0\n",
    "        error_node = 0\n",
    "\n",
    "        for c in com:\n",
    "            for v in com_node[c]:\n",
    "                  for u in com_node[c]:\n",
    "                    if v == u:\n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            if u in neigh[v]:\n",
    "                                a = 1\n",
    "                            else:\n",
    "                                a = 0\n",
    "                            ov = len(node_com[v])\n",
    "                            ou = len(node_com[u])\n",
    "                            kv = len(neigh[v])\n",
    "                            ku = len(neigh[u])\n",
    "                            score = score + (1 / (ov * ou) * (a - (kv * ku) / (2 * m)))\n",
    "                        except Exception as e:\n",
    "                            print(\"节点{}错误\".format(v))\n",
    "                            error_node = error_node + 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            pass\n",
    "        print(\"错误节点数{}\".format(error_node))\n",
    "        score = float(score / (2 * m))\n",
    "        return score\n",
    "    def get_com_node(self,partiton):\n",
    "        com_node={}\n",
    "        for i ,j in enumerate(partiton):\n",
    "            com_node[i]=j\n",
    "        return com_node\n",
    "    def com_node_reversal(self,com_node):\n",
    "        node_com = {}\n",
    "        print(\"转化为node_community形式\")\n",
    "        for k ,v in com_node.items():\n",
    "            for node in v:\n",
    "                s=[]\n",
    "                if node in node_com.keys():\n",
    "                    if k in node_com[node]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        s = node_com[node]\n",
    "                        node_com[node] = s + [k]\n",
    "\n",
    "\n",
    "                else:\n",
    "                    s=[k]\n",
    "                    node_com[node]=s\n",
    "        return node_com\n",
    "    def Nodeneigh(self,G):\n",
    "        node_attr={}\n",
    "        for n in list(G.nodes()):\n",
    "            nei=list(G.neighbors(n))\n",
    "            # nei=list(G.successors(n))+list(G.predecessors(n))\n",
    "            # print(len(nei))\n",
    "            # n=int(n)\n",
    "            node_attr[n]=nei\n",
    "        return node_attr\n",
    "\n",
    "def compare_demo(G,f):\n",
    "    e=Emodularity()\n",
    "    r_l=[0.25,0.5]\n",
    "    m_score=[]\n",
    "    for i in r_l:\n",
    "        i_j={}\n",
    "        for j in range(1):\n",
    "            g=G.copy()\n",
    "            c={}\n",
    "            st=time.time()\n",
    "            c_cdlib=algorithms.demon(g,epsilon=i)\n",
    "            sp=time.time()\n",
    "            readwrite.write_community_csv(c_cdlib,f+str(i)+'_'+str(j)+'.csv')\n",
    "            m_cdlib=ev.newman_girvan_modularity(g,c_cdlib).score\n",
    "            # p=ev.conductance(g,c_cdlib).score\n",
    "            o=e.emodularity(g,c_cdlib)\n",
    "            # print(p)\n",
    "            # print(m_cdlib)\n",
    "            c['modularity']=m_cdlib\n",
    "            # c['conduance']=p\n",
    "            c['over_modularity']=o\n",
    "            c['time']=sp-st\n",
    "            c['com_count']=len(c_cdlib.communities)\n",
    "            c['iter']=j\n",
    "            print(c)\n",
    "            i_j[j]=c\n",
    "        dt=pd.DataFrame(i_j)\n",
    "        dt1 = dt.stack()\n",
    "        dt2 = dt1.unstack(0)\n",
    "        dt2['r']=[i]*len(dt2.index)\n",
    "        dt2.to_excel(f + g.name+'_calculate_output_{}.xlsx'.format(i), index=True, header=True)\n",
    "        m_score.append(dt2)\n",
    "    m_score=pd.concat(m_score)\n",
    "    m_score.to_excel(f+'all_cal.xlsx')\n",
    "def make_filedir(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.mkdir(filepath)\n",
    "    return filepath\n",
    "\n",
    "fo='work/lfm/'\n",
    "fi='work/aps/aps/APS_network/aps_article_network/article_journal_network/'\n",
    "g_list=['PRSTPER','PRI','PRMATERIALS','RMP','PRAPPLIED','PRX','PRSTAB','PRPER','PRFLUIDS','PRAB','PRE','PR','PRB','PRC','PRA','PRL','PRD']\n",
    "#\n",
    "# g_list = []\n",
    "for g in g_list:\n",
    "    f=make_filedir(fo+g+'/')\n",
    "    G_file=fi+g+'.gexf'\n",
    "    G = nx.read_gexf(G_file, node_type=str)\n",
    "    # 判断图的类型\n",
    "    if nx.is_directed(G):\n",
    "        G = G.to_undirected()\n",
    "    s=list(nx.isolates(G))\n",
    "    G.remove_nodes_from(s)\n",
    "    compare_demo(G,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare for SCI, the original algorithm written by should refer to \n",
    "#Wang, X., Jin, D., Cao, X., Yang, L., & Zhang, W. (2016). Semantic community identification in large attribute networks. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI’16). AAAI Press, 265–271.\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import json\n",
    "import pandas as pd\n",
    "from cdlib import *\n",
    "from cdlib import evaluation,readwrite\n",
    "def read_edge(fe):\n",
    "    edgelist=[]\n",
    "    with open(fe,'w',encoding='utf-8')as f:\n",
    "        for e in f.readlines():\n",
    "            e=e.strip('\\n').split(' ')\n",
    "            edgelist.append(e)\n",
    "    edgelist=np.asarray(edgelist)\n",
    "    return edgelist\n",
    "def read_sentence(fv):\n",
    "    sen={}\n",
    "    with open(fv,'r',encoding='utf-8')as f:\n",
    "        for w in f.readlines():\n",
    "            w=w.strip('\\n').split(':')\n",
    "            sen[w[0]]=w[1]\n",
    "    return sen\n",
    "def word_vector(texts):\n",
    "    cv = CountVectorizer(binary=True)\n",
    "    document_vec = cv.fit_transform(texts)\n",
    "    v=document_vec.toarray()\n",
    "    return v\n",
    "def main():\n",
    "    f_g=''\n",
    "    f_v=''\n",
    "    f_s=''\n",
    "    f_gs=''\n",
    "    # 'PRMATERIALS',\n",
    "    g_list = ['PRSTPER', 'PRI', 'RMP', 'PRAPPLIED', 'PRX', 'PRSTAB',\n",
    "              'PRPER', 'PRFLUIDS', 'PRAB', 'PRE', 'PR', 'PRB', 'PRC', 'PRA', 'PRL', 'PRD']\n",
    "\n",
    "    for g in g_list:\n",
    "        data={}\n",
    "        fg=f_g+g+'.gexf'\n",
    "        fv=f_v+g+'.txt'\n",
    "        data_save=f_s+g+'.mat'\n",
    "        fgs=f_gs+g+'.gexf'\n",
    "        G=nx.read_gexf(fg)\n",
    "        G1=nx.convert_node_labels_to_integers(G,first_label=1,label_attribute=True)\n",
    "        edge_list=[]\n",
    "        l=nx.selfloop_edges(G1)\n",
    "        G1.remove_edges_from(l)\n",
    "        nx.write_gexf(G1,fgs)\n",
    "        for line in nx.generate_edgelist(G1, data=False):\n",
    "            edge_list.append(list(line))\n",
    "        edge_list=np.asarray(edge_list)\n",
    "        data['adj']=edge_list\n",
    "        sen=read_sentence(fv)\n",
    "        sen_list=[]\n",
    "        for n in list(G1.nodes()):\n",
    "            n_label=G1.nodes[n]['label_attribute']\n",
    "            sen_list.append(sen[n_label])\n",
    "        content_vector=word_vector(sen_list)\n",
    "        data['content']=content_vector\n",
    "        scio.savemat(data_save,data)\n",
    "def cal_sci_Q():\n",
    "    g_list = ['PRSTPER', 'PRI', 'RMP', 'PRAPPLIED', 'PRX', 'PRSTAB',\n",
    "              'PRPER', 'PRFLUIDS', 'PRAB', 'PRE', 'PR', 'PRB', 'PRC', 'PRA', 'PRL', 'PRD']\n",
    "    f=''\n",
    "    f_g=''\n",
    "    f_o=''\n",
    "    calcul={}\n",
    "    for g in g_list:\n",
    "        cal={}\n",
    "        fn=f+g+'_output.mat'\n",
    "        fg=f_g+g+'.gexf'\n",
    "        G=nx.read_gexf(fg)\n",
    "        data=scio.loadmat(fn)\n",
    "        # Ucontent, res, U1index\n",
    "        com=data['res']\n",
    "        comms={}\n",
    "        nodes={}\n",
    "        for n,c in enumerate(com):\n",
    "            nodes[n+1]=c\n",
    "        for k ,v in nodes.items():\n",
    "            if v in comms.keys():\n",
    "                comms[v].append(k)\n",
    "            else:\n",
    "                comms[v]=[k]\n",
    "        com_size={}\n",
    "        for i,j in comms.items():\n",
    "            com_size[i]=len(j)\n",
    "        with open(f_o+g+'_comsize.json','w',encoding='utf-8')as f1:\n",
    "            json.dump(com_size,f1)\n",
    "        community=list(comms.values())\n",
    "        partition= NodeClustering(community, G, \"SCI\", method_parameters={\"T\": 100, \"r\": 0.25}, overlap=False)\n",
    "        readwrite.write_community_csv(partition,f_o+g+'_com.csv')\n",
    "        cal['modularity']=evaluation.newman_girvan_modularity(G,partition).score\n",
    "        cal['com']=len(comms)\n",
    "        calcul[g]=cal\n",
    "    calcul=pd.DataFrame(calcul)\n",
    "    calcul.to_excel(f_o+'calculate_sci.xlsx')\n",
    "    print(calcul)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infomap refer to mapequation.org/infomap/#Install\n",
    "#OSLOM refer to oslom.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LPA\n",
    "from cdlib import algorithms,evaluation as ev,readwrite\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "def compare_lpa(G,f):\n",
    "    i_j=[]\n",
    "    for j in range(10):\n",
    "        g=G.copy()\n",
    "        c={}\n",
    "        st=time.time()\n",
    "        c_cdlib=algorithms.label_propagation(g)\n",
    "        sp=time.time()\n",
    "        readwrite.write_community_csv(c_cdlib,f+'_'+str(j)+'.csv')\n",
    "        m_cdlib=ev.newman_girvan_modularity(g,c_cdlib).score\n",
    "        p=ev.conductance(g,c_cdlib).score\n",
    "        print(p)\n",
    "        print(m_cdlib)\n",
    "        c['modularity']=m_cdlib\n",
    "        c['conduance']=p\n",
    "        c['time']=sp-st\n",
    "        c['com_count']=len(c_cdlib.communities)\n",
    "        c['iter']=j\n",
    "        i_j.append(c)\n",
    "    dt=pd.DataFrame(i_j)\n",
    "    dt.to_excel(f + '{}.xlsx'.format(g.name), index=True, header=True)\n",
    "def make_filedir(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.mkdir(filepath)\n",
    "    return filepath\n",
    "fo='F:/aps_analysis/lpa/'\n",
    "fi= 'F:/aps_out/aps/APS_network/aps_article_network/article_journal_network/'\n",
    "g_list=['PRSTPER','PRI','PRMATERIALS','RMP','PRAPPLIED','PRX','PRSTAB','PRPER','PRFLUIDS','PRAB','PRE','PR','PRB','PRC','PRA','PRL','PRD']\n",
    "# 'PRB',\n",
    "# g_list = []\n",
    "for g in g_list:\n",
    "    f=make_filedir(fo+g+'/')\n",
    "    G_file=fi+g+'.gexf'\n",
    "    G = nx.read_gexf(G_file, node_type=str)\n",
    "    # 判断图的类型\n",
    "    if nx.is_directed(G):\n",
    "        G = G.to_undirected()\n",
    "    s=list(nx.isolates(G))\n",
    "    G.remove_nodes_from(s)\n",
    "    compare_lpa(G,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Emodularity():\n",
    "    def emodularity(self, G, partition):\n",
    "        #com_node: {com:[node_id]}, neigh:  {node_id:[node_id,node_id]}, node_com  {node_id:[com_id]}\n",
    "        comm=partition.communities\n",
    "        com_node=self.get_com_node(comm)\n",
    "        node_com=self.com_node_reversal(com_node)\n",
    "        neigh=self.Nodeneigh(G)\n",
    "        com = com_node.keys()\n",
    "        m = float(G.number_of_edges())\n",
    "        score = 0\n",
    "        error_node = 0\n",
    "\n",
    "        for c in com:\n",
    "            for v in com_node[c]:\n",
    "                  for u in com_node[c]:\n",
    "                    if v == u:\n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            if u in neigh[v]:\n",
    "                                a = 1\n",
    "                            else:\n",
    "                                a = 0\n",
    "                            ov = len(node_com[v])\n",
    "                            ou = len(node_com[u])\n",
    "                            kv = len(neigh[v])\n",
    "                            ku = len(neigh[u])\n",
    "                            score = score + (1 / (ov * ou) * (a - (kv * ku) / (2 * m)))\n",
    "                        except Exception as e:\n",
    "                            print(\"节点{}错误\".format(v))\n",
    "                            error_node = error_node + 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            pass\n",
    "        print(\"错误节点数{}\".format(error_node))\n",
    "        score = float(score / (2 * m))\n",
    "        return score\n",
    "    def get_com_node(self,partiton):\n",
    "        com_node={}\n",
    "        for i ,j in enumerate(partiton):\n",
    "            com_node[i]=j\n",
    "        return com_node\n",
    "    def com_node_reversal(self,com_node):\n",
    "        node_com = {}\n",
    "        print(\"转化为node_community形式\")\n",
    "        for k ,v in com_node.items():\n",
    "            for node in v:\n",
    "                s=[]\n",
    "                if node in node_com.keys():\n",
    "                    if k in node_com[node]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        s = node_com[node]\n",
    "                        node_com[node] = s + [k]\n",
    "\n",
    "\n",
    "                else:\n",
    "                    s=[k]\n",
    "                    node_com[node]=s\n",
    "        return node_com\n",
    "    def Nodeneigh(self,G):\n",
    "        node_attr={}\n",
    "        for n in list(G.nodes()):\n",
    "            nei=list(G.neighbors(n))\n",
    "            # nei=list(G.successors(n))+list(G.predecessors(n))\n",
    "            # print(len(nei))\n",
    "            # n=int(n)\n",
    "            node_attr[n]=nei\n",
    "        return node_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iLOUVAIN\n",
    "#ilouvain\n",
    "import random\n",
    "from cdlib import algorithms,evaluation as ev,readwrite\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "class Emodularity():\n",
    "    def emodularity(self, G, partition):\n",
    "        #com_node: {com:[node_id]}, neigh:  {node_id:[node_id,node_id]}, node_com  {node_id:[com_id]}\n",
    "        comm=partition.communities\n",
    "        com_node=self.get_com_node(comm)\n",
    "        node_com=self.com_node_reversal(com_node)\n",
    "        neigh=self.Nodeneigh(G)\n",
    "        com = com_node.keys()\n",
    "        m = float(G.number_of_edges())\n",
    "        score = 0\n",
    "        error_node = 0\n",
    "\n",
    "        for c in com:\n",
    "            for v in com_node[c]:\n",
    "                  for u in com_node[c]:\n",
    "                    if v == u:\n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            if u in neigh[v]:\n",
    "                                a = 1\n",
    "                            else:\n",
    "                                a = 0\n",
    "                            ov = len(node_com[v])\n",
    "                            ou = len(node_com[u])\n",
    "                            kv = len(neigh[v])\n",
    "                            ku = len(neigh[u])\n",
    "                            score = score + (1 / (ov * ou) * (a - (kv * ku) / (2 * m)))\n",
    "                        except Exception as e:\n",
    "                            print(\"节点{}错误\".format(v))\n",
    "                            error_node = error_node + 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            pass\n",
    "        print(\"错误节点数{}\".format(error_node))\n",
    "        score = float(score / (2 * m))\n",
    "        return score\n",
    "    def get_com_node(self,partiton):\n",
    "        com_node={}\n",
    "        for i ,j in enumerate(partiton):\n",
    "            com_node[i]=j\n",
    "        return com_node\n",
    "    def com_node_reversal(self,com_node):\n",
    "        node_com = {}\n",
    "        print(\"转化为node_community形式\")\n",
    "        for k ,v in com_node.items():\n",
    "            for node in v:\n",
    "                s=[]\n",
    "                if node in node_com.keys():\n",
    "                    if k in node_com[node]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        s = node_com[node]\n",
    "                        node_com[node] = s + [k]\n",
    "\n",
    "\n",
    "                else:\n",
    "                    s=[k]\n",
    "                    node_com[node]=s\n",
    "        return node_com\n",
    "    def Nodeneigh(self,G):\n",
    "        node_attr={}\n",
    "        for n in list(G.nodes()):\n",
    "            nei=list(G.neighbors(n))\n",
    "            # nei=list(G.successors(n))+list(G.predecessors(n))\n",
    "            # print(len(nei))\n",
    "            # n=int(n)\n",
    "            node_attr[n]=nei\n",
    "        return node_attr\n",
    "\n",
    "def read_att(fi):\n",
    "    data={}\n",
    "    with open(fi,'r',encoding='utf-8')as f:\n",
    "        for c in f.readlines():\n",
    "            c=c.strip('\\n').split(':')\n",
    "            #由于原始数据存入时有bug,多写了一个join，因此造成每个数字间有三个空格。在此处需要注意\n",
    "            l=c[1].split('   ')\n",
    "#             print(l)\n",
    "            l=list(map(float,l))\n",
    "            dt={}\n",
    "            for i,j in enumerate(l):\n",
    "#                 print(type(j))\n",
    "                dt[i]=j\n",
    "            data[c[0]]=dt\n",
    "    return data\n",
    "\n",
    "def compare_ilouvain(G,label,f):\n",
    "    labels={}\n",
    "    node_id={}\n",
    "    #防止某些节点不在label中\n",
    "    for n in list(G.nodes()):\n",
    "        labels[n]=label[n]\n",
    "        node_id[n]=n\n",
    "    \n",
    "    \n",
    "    e=Emodularity()\n",
    "#     r_l=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "    r_l=[1]\n",
    "    m_score=[]\n",
    "    for i in r_l:\n",
    "        i_j={}\n",
    "        for j in range(1):\n",
    "            g=G.copy()\n",
    "            c={}\n",
    "            st=time.time()\n",
    "            c_cdlib=algorithms.ilouvain(g, labels, node_id)\n",
    "            sp=time.time()\n",
    "            readwrite.write_community_csv(c_cdlib,f+str(g.name)+'_'+str(i)+'.csv')\n",
    "            m_cdlib=ev.link_modularity(g,c_cdlib).score\n",
    "            p=ev.newman_girvan_modularity(g,c_cdlib).score\n",
    "            o=e.emodularity(g,c_cdlib)\n",
    "            print(o)\n",
    "#             print(m_cdlib)\n",
    "            c['link_modularity']=m_cdlib\n",
    "            c['modularity']=p\n",
    "            c['over_modularity']=o\n",
    "            c['time']=sp-st\n",
    "            c['com_count']=len(c_cdlib.communities)\n",
    "            c['iter']=j\n",
    "            print(c)\n",
    "            i_j[j]=c\n",
    "        dt=pd.DataFrame(i_j)\n",
    "        dt1 = dt.stack()\n",
    "        dt2 = dt1.unstack(0)\n",
    "        dt2['r']=[i]*len(dt2.index)\n",
    "        dt2.to_excel(f + '{}_{}_cal.xlsx'.format(g.name,i), index=True, header=True)\n",
    "        m_score.append(dt2)\n",
    "    m_score=pd.concat(m_score)\n",
    "    m_score.to_excel(f+'all_cal.xlsx')\n",
    "def make_filedir(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        os.mkdir(filepath)\n",
    "    return filepath  \n",
    "fgs='F:/aps_analysis/aps_compare_dcm/graph/'\n",
    "fts='F:/aps_analysis/aps_compare_dcm/count_dict/'\n",
    "fo='F:/aps_analysis/aps_compare_ilouvain/'\n",
    "g_list=[\n",
    "# 'PRPER','PRAB','PRFLUIDS','PRI','PRSTPER','PRAPPLIED','PRX',\n",
    "        'PRSTAB'\n",
    "#         ,'RMP','PRE','PR','PRC','PRA','PRD','PRL','PRB'\n",
    "       ]\n",
    "# g_list=['PRSTPER']\n",
    "for g in g_list:\n",
    "    print(g)\n",
    "    ft=fts+g+'_count_dict.dsm'\n",
    "    fg=fgs+g+'.gexf'\n",
    "    node_label=read_att(ft)\n",
    "    G = nx.read_gexf(fg)\n",
    "    # 判断图的类型,G在上一步已经去掉self_loop,节点标签转换为1开始的数字，不清楚是否有向变无向\n",
    "    #和去掉度为零的节点，因此在这里重新处理\n",
    "    if nx.is_directed(G):\n",
    "        G = G.to_undirected()\n",
    "    s=list(nx.isolates(G))\n",
    "    G.remove_nodes_from(s)\n",
    "    compare_ilouvain(G,node_label,fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from cdlib import evaluation, readwrite\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plta\n",
    "from cdlib import  algorithms,evaluation\n",
    "from cdlib import *\n",
    "import json\n",
    "\n",
    "\"\"\"需要获取节点的邻居节点，节点度，节点拥有的社区，节点的社区数\"\"\"\n",
    "class EQ():\n",
    "    def __init__(self,filepath):\n",
    "        self.filepath=filepath\n",
    "    def Nodeneigh(self,G):\n",
    "        node_attr={}\n",
    "        print(\"读取节点邻居节点\")\n",
    "        for n in list(G.nodes()):\n",
    "            # print(\"节点{}的度为{}\".format(n,G.degree(n)))\n",
    "            # print(list(G.successors(n)))\n",
    "            # print(list(G.neighbors(n)))\n",
    "            # print(list(G.predecessors(n)))\n",
    "            nei=list(G.neighbors(n))\n",
    "            # nei=list(G.successors(n))+list(G.predecessors(n))\n",
    "            # print(len(nei))\n",
    "            # n=int(n)\n",
    "            # node_attr[n]=list(map(int,nei))\n",
    "            node_attr[n] =  nei\n",
    "        # print(node_attr)\n",
    "        return node_attr\n",
    "    def read_comnode(self,filename):\n",
    "        print(\"读取社区划分结果\")\n",
    "        com_node = {}\n",
    "        node_list=[]\n",
    "        i = 0\n",
    "        with open(filename, 'r', encoding='utf8') as f:\n",
    "            for r in f.readlines():\n",
    "                r = r.strip().strip('\\n').strip('\\t')\n",
    "                if r is not None:\n",
    "                    n_list=list(map(int, r.split()))\n",
    "                    com_node[i] = n_list\n",
    "                    node_list=node_list+n_list\n",
    "                    i = i + 1\n",
    "                else:\n",
    "                    continue\n",
    "        print(\"社区数量{}\".format(i))\n",
    "\n",
    "        return com_node\n",
    "    def com_node_reversal(self,com_node):\n",
    "        node_com = {}\n",
    "        print(\"转化为node_community形式\")\n",
    "        for k ,v in com_node.items():\n",
    "            for node in v:\n",
    "                s=[]\n",
    "                if node in node_com.keys():\n",
    "                    if k in node_com[node]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        s = node_com[node]\n",
    "                        node_com[node] = s + [k]\n",
    "\n",
    "\n",
    "                else:\n",
    "                    s=[k]\n",
    "                    node_com[node]=s\n",
    "        return node_com\n",
    "    def Emodularity(self,G,com_node,n_attr):\n",
    "        print(\"计算EQ\")\n",
    "        print(\"图中节点数{}\".format(len(n_attr)))\n",
    "        # com_node=self.read_comnode(filename)\n",
    "\n",
    "        node_com=self.com_node_reversal(com_node)\n",
    "        print(\"社团划分包括节点数{}\".format(len(node_com.keys())))\n",
    "        com=com_node.keys()\n",
    "        m=float(G.number_of_edges())\n",
    "        nodes=list(node_com.keys())\n",
    "\n",
    "        print(\"交集数量为\")\n",
    "        print(len(list(set(n_attr.keys()) & set(nodes))))\n",
    "        node_attr = n_attr\n",
    "        score=0\n",
    "        error_node=0\n",
    "        count=0\n",
    "        for c in com:\n",
    "            print(\"社区{}\".format(c))\n",
    "            for v in com_node[c]:\n",
    "                count=count+1\n",
    "                print(\"开始遍历第{}个节点,还有{}个节点\".format(count,(len(nodes)-count)))\n",
    "                for u in com_node[c]:\n",
    "                    if v==u:\n",
    "                        pass\n",
    "                    else:\n",
    "                        # print(node_attr[v])\n",
    "                        try:\n",
    "                            if u in node_attr[v]:\n",
    "                                a = 1\n",
    "                            else:\n",
    "                                a = 0\n",
    "                            ov = len(node_com[v])\n",
    "                            ou = len(node_com[u])\n",
    "                            kv = len(node_attr[v])\n",
    "                            ku = len(node_attr[u])\n",
    "                            score = score + (1 / (ov * ou) * (a - (kv * ku) / (2 * m)))\n",
    "                        except Exception as e:\n",
    "                                print(\"节点{}错误\".format(v))\n",
    "                                error_node=error_node+1\n",
    "                                continue\n",
    "                        else:\n",
    "                            pass\n",
    "        print(\"错误节点数{}\".format(error_node))\n",
    "        score=float(score/(2*m))\n",
    "        return score\n",
    "    def score_number(self,g_list):\n",
    "        \n",
    "        f_g='F:/aps_analysis/graph/edgelist_int/'\n",
    "        f_c='F:/aps_analysis/aps_compare_oslom/COM/'\n",
    "        f_o='F:/aps_analysis/aps_compare_oslom/output/'\n",
    "        d={}\n",
    "        for g in g_list:\n",
    "            c={}\n",
    "            fg=f_g+g+'.gexf'\n",
    "            fc=f_c+g\n",
    "            fo=f_o+g+'community.txt'\n",
    "            G=nx.read_gexf(fg)\n",
    "            node_attr=self.Nodeneigh(G)\n",
    "\n",
    "            com,com_size=self.read_infomap_com(fc)\n",
    "            self.write_dict_list(com,fo)\n",
    "            c['modularity']=self.Emodularity(G,com,node_attr)\n",
    "            comms=list(com.values())\n",
    "\n",
    "            partition = NodeClustering(comms, G, \"infomap\", method_parameters={\"T\": 100, \"r\": 0.25}, overlap=True)\n",
    "\n",
    "            c['conduance']=evaluation.conductance(G,partition).score\n",
    "            c['newman_modularity']=evaluation.newman_girvan_modularity(G,partition).score\n",
    "            c['com_count']=len(com.keys())\n",
    "            d[g]=c\n",
    "            readwrite.write_community_csv(partition,fo+'community.csv')\n",
    "            with open(fo+'_com_node_count.json','w',encoding='utf-8')as f:\n",
    "                json.dump(com_size,f)\n",
    "        data=pd.DataFrame(d)\n",
    "        data.to_excel(f_o+'infomap_cal.xlsx')\n",
    "        return data\n",
    "    def read_G(self,f,textname):\n",
    "        G = nx.DiGraph()\n",
    "        l=[]\n",
    "        with open(f+textname,'r')as f:\n",
    "           for s in f.readlines():\n",
    "               s=s.strip('\\n').strip('\\t')\n",
    "               n=list(s.split(' '))\n",
    "               l.append(n)\n",
    "\n",
    "        G.add_weighted_edges_from(l)\n",
    "        print(G.number_of_nodes())\n",
    "        print(G.number_of_edges())\n",
    "        return G\n",
    "\n",
    "    def read_infomap_com(self,f):\n",
    "        f=f+'.dat.clu'\n",
    "        comms = {}\n",
    "        com_size = {}\n",
    "        with open(f , 'r', encoding='utf-8')as f1:\n",
    "            i = 0\n",
    "            for c in f1.readlines():\n",
    "                i = i + 1\n",
    "                if i < 10:\n",
    "                    pass\n",
    "                else:\n",
    "                    node = c.strip('\\n').split(' ')[0]\n",
    "                    com = c.strip('\\n').split(' ')[1]\n",
    "                    if com in comms.keys():\n",
    "                        comms[com].append(node)\n",
    "                    else:\n",
    "                        comms[com] = [node]\n",
    "            comms = dict(sorted(comms.items(), reverse=False, key=lambda k: k[1]))\n",
    "            for k,v in comms.items():\n",
    "                com_size[k]=len(v)\n",
    "            # self.write_dict_list(com_size,f+s+'_comsize.txt')\n",
    "\n",
    "\n",
    "        return comms,com_size\n",
    "\n",
    "    def read_com(self,fn):\n",
    "        com = {}\n",
    "        com_size = {}\n",
    "        i = 0\n",
    "        with open(fn, 'r', encoding='utf-8')as f:\n",
    "            for c in f.readlines():\n",
    "\n",
    "                if '#' in c:\n",
    "                    pass\n",
    "                else:\n",
    "                    n = c.lstrip(' ').rstrip(' ').strip('\\n').split(' ')\n",
    "                    n=[k for k in n if k !='']\n",
    "                    print(n)\n",
    "                    com[i] = n\n",
    "                    com_size[i] = len(n)\n",
    "                    i = i + 1\n",
    "        com = dict(sorted(com.items(), reverse=False, key=lambda k: k[1]))\n",
    "        return com, com_size\n",
    "    def write_dict_list(self,d_list,filename):\n",
    "        with open(filename,'w',encoding='utf-8')as f:\n",
    "            for k ,v in d_list.items():\n",
    "                f.write(str(k)+':'+' '.join(v))\n",
    "                f.write('\\n')\n",
    "        return\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    g_list = ['PRSTPER', 'PRI', 'PRMATERIALS', 'RMP', 'PRAPPLIED', 'PRX', 'PRSTAB',\n",
    "              'PRPER', 'PRFLUIDS', 'PRAB', 'PRE', 'PR', 'PRB', 'PRC', 'PRA', 'PRL', 'PRD']\n",
    "    f_o = 'F:/aps_analysis/aps_compare_oslom/output/'\n",
    "    e=EQ(f_o)\n",
    "    e.score_number(g_list)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
